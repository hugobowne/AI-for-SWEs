{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LinkedIn Profile Extraction and Validation Workflow\n",
    "\n",
    "## Introduction: Testing Outputs of Nondeterministic Software\n",
    "\n",
    "In this Lightning Lesson, we will focus on testing the outputs of nondeterministic software, specifically Large Language Models (LLMs). While we'll use a recruitment example—extracting structured data from LinkedIn profiles and automating email outreach—the main goal is to understand how to systematically evaluate outputs from software that does not always behave predictably.\n",
    "\n",
    "This process involves multiple layers of testing, which are crucial in ensuring that the final application meets business goals and delivers reliable results. The layers include:\n",
    "\n",
    "1. **Validating Structured Output**: The first step is ensuring that the LLM returns the correct structured data. In this case, we extract information like the candidate's name, current role, location, previous roles, and education from their LinkedIn profile. The accuracy of this data is essential for the next steps in the process.\n",
    "\n",
    "2. **Ensuring Email Quality**: After extracting structured data, the next step is generating automated outreach emails. These emails need to look good and require domain expertise to verify their content. For example, ensuring the tone is professional and the content is relevant to the business context is critical.\n",
    "\n",
    "3. **Achieving Business Goals**: Ultimately, the goal is to ensure that the outreach leads to replies and successfully recruits quality candidates. This means verifying that the entire flow—from profile extraction to email generation—aligns with business goals and results in effective engagement.\n",
    "\n",
    "This lesson will focus specifically on the first layer: validating the result of the LLM call. We'll explore how to test and refine LLM outputs before moving on to more complex evaluations in later stages of development.\n",
    "\n",
    "This notebook walks through a workflow for extracting structured data from LinkedIn profiles using an LLM, validating the outputs, and preparing the data for domain expert review. The key steps include:\n",
    "\n",
    "1. Extracting structured JSON data from unstructured LinkedIn text.\n",
    "2. Validating the JSON output.\n",
    "3. Flagging suspicious profiles based on missing or invalid fields.\n",
    "4. Saving the data to a CSV file.\n",
    "5. Reviewing and annotating the profiles in an interactive table.\n",
    "\n",
    "This process can be applied to LinkedIn profiles for individuals or companies to assist with data validation, annotation, and review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Testing the Pipeline with One Profile\n",
    "\n",
    "Before applying the process to all profiles, we start by testing the pipeline with a single LinkedIn profile. This ensures that:\n",
    "- The LLM generates structured JSON data.\n",
    "- The JSON output is valid and complete.\n",
    "\n",
    "We'll use one text file (`<profile_name>.txt`) as input and observe the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the file content into a string\n",
    "file_path = \"data/hbaLI.txt\"  # Replace with your actual file path\n",
    "\n",
    "# Open the file in read mode and read the content\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    linkedin_text = file.read()\n",
    "\n",
    "# Print the content to verify\n",
    "# print(linkedin_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"Name\": \"Hugo Bowne-Anderson\",\n",
      "  \"Current Role\": \"Independent Data and AI Scientist, Consultant, Writer, Educator, Podcaster\",\n",
      "  \"Location\": \"Darlinghurst, New South Wales, Australia\",\n",
      "  \"Previous Roles\": [\n",
      "    {\n",
      "      \"Title\": \"Head of Developer Relations\",\n",
      "      \"Company\": \"Outerbounds\",\n",
      "      \"Duration\": \"Feb 2022 - Aug 2024\"\n",
      "    },\n",
      "    {\n",
      "      \"Title\": \"Head of Data Science Evangelism and Marketing\",\n",
      "      \"Company\": \"Coiled\",\n",
      "      \"Duration\": \"May 2020 - Oct 2021\"\n",
      "    },\n",
      "    {\n",
      "      \"Title\": \"Data Scientist\",\n",
      "      \"Company\": \"DataCamp\",\n",
      "      \"Duration\": \"Sep 2017 - May 2020\"\n",
      "    },\n",
      "    {\n",
      "      \"Title\": \"Curriculum Engineer (Python)\",\n",
      "      \"Company\": \"DataCamp\",\n",
      "      \"Duration\": \"Mar 2016 - May 2020\"\n",
      "    },\n",
      "    {\n",
      "      \"Title\": \"Postdoctoral Associate/Writer\",\n",
      "      \"Company\": \"Yale University\",\n",
      "      \"Duration\": \"2013 - Mar 2016\"\n",
      "    }\n",
      "  ],\n",
      "  \"Education\": [\n",
      "    {\n",
      "      \"Institution\": \"UNSW\",\n",
      "      \"Degree\": \"Doctor of Philosophy (PhD)\",\n",
      "      \"Field\": \"Pure Mathematics\",\n",
      "      \"Duration\": \"2006 - 2011\"\n",
      "    },\n",
      "    {\n",
      "      \"Institution\": \"University of Sydney\",\n",
      "      \"Degree\": \"Bachelor of Science (B.S.) (First Class Honors)\",\n",
      "      \"Field\": \"Mathematics, English Literature\",\n",
      "      \"Duration\": \"2001 - 2005\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = 'XXX'\n",
    "client = OpenAI()\n",
    "\n",
    "\n",
    "# Define the user message (chat-style format)\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": f\"\"\"\n",
    "Extract the following structured information from the text below:\n",
    "- Name\n",
    "- Current Role\n",
    "- Location\n",
    "- Previous Roles\n",
    "- Education\n",
    "\n",
    "Text: {linkedin_text}\n",
    "\n",
    "Output the result as a JSON object.\n",
    "\"\"\"}\n",
    "]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=messages\n",
    ")\n",
    "output = response.choices[0].message.content\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid JSON: Expecting value: line 1 column 1 (char 0)\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Test the output\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[43mtest_valid_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m, in \u001b[0;36mtest_valid_json\u001b[0;34m(output)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtest_valid_json\u001b[39m(output):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 5\u001b[0m         parsed_output \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(parsed_output, \u001b[38;5;28mdict\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput is not a valid JSON object\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValid JSON!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/json/decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscan_once(s, idx)\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def test_valid_json(output):\n",
    "    try:\n",
    "        parsed_output = json.loads(output)\n",
    "        assert isinstance(parsed_output, dict), \"Output is not a valid JSON object\"\n",
    "        print(\"Valid JSON!\")\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Invalid JSON: {e}\")\n",
    "        raise\n",
    "\n",
    "# Test the output\n",
    "test_valid_json(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Switching to OpenAI JSON Mode\n",
    "\n",
    "Using OpenAI's JSON mode ensures that the output is structured as valid JSON by design. This step improved reliability and reduced the need for custom validation logic. We:\n",
    "- Re-ran the API call with `response_format={\"type\": \"json_object\"}`.\n",
    "- Validated that the output adheres to the required JSON structure.\n",
    "\n",
    "Below is the implementation of JSON mode with a single test profile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"Name\": \"Hugo Bowne-Anderson\",\n",
      "  \"Current Role\": \"Independent Data and AI Scientist\",\n",
      "  \"Location\": \"Darlinghurst, New South Wales, Australia\",\n",
      "  \"Previous Roles\": [\n",
      "    {\n",
      "      \"Title\": \"Head of Developer Relations\",\n",
      "      \"Company\": \"Outerbounds\",\n",
      "      \"Duration\": \"Feb 2022 - Aug 2024\"\n",
      "    },\n",
      "    {\n",
      "      \"Title\": \"Head of Marketing and Data Science Evangelism\",\n",
      "      \"Company\": \"Coiled\",\n",
      "      \"Duration\": \"May 2020 - Oct 2021\"\n",
      "    },\n",
      "    {\n",
      "      \"Title\": \"Data Scientist\",\n",
      "      \"Company\": \"DataCamp\",\n",
      "      \"Duration\": \"Sep 2017 - May 2020\"\n",
      "    },\n",
      "    {\n",
      "      \"Title\": \"Curriculum Engineer (Python)\",\n",
      "      \"Company\": \"DataCamp\",\n",
      "      \"Duration\": \"Mar 2016 - May 2020\"\n",
      "    },\n",
      "    {\n",
      "      \"Title\": \"Postdoctoral Associate/Writer\",\n",
      "      \"Company\": \"Yale University\",\n",
      "      \"Duration\": \"2013 - Mar 2016\"\n",
      "    }\n",
      "  ],\n",
      "  \"Education\": [\n",
      "    {\n",
      "      \"Degree\": \"Doctor of Philosophy (PhD)\",\n",
      "      \"Field\": \"Pure Mathematics\",\n",
      "      \"Institution\": \"UNSW\",\n",
      "      \"Duration\": \"2006 - 2011\"\n",
      "    },\n",
      "    {\n",
      "      \"Degree\": \"Bachelor of Science (B.S.) with First Class Honors\",\n",
      "      \"Fields\": \"Mathematics, English Literature\",\n",
      "      \"Institution\": \"University of Sydney\",\n",
      "      \"Duration\": \"2001 - 2005\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    response_format={ \"type\": \"json_object\"},\n",
    "    messages=messages\n",
    ")\n",
    "output = response.choices[0].message.content\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid JSON!\n"
     ]
    }
   ],
   "source": [
    "# Test the output\n",
    "test_valid_json(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Name': 'Hugo Bowne-Anderson', 'Current Role': 'Independent Data and AI Scientist', 'Location': 'Darlinghurst, New South Wales, Australia', 'Previous Roles': [{'Title': 'Head of Developer Relations', 'Company': 'Outerbounds', 'Duration': 'Feb 2022 - Aug 2024'}, {'Title': 'Head of Marketing and Data Science Evangelism', 'Company': 'Coiled', 'Duration': 'May 2020 - Oct 2021'}, {'Title': 'Data Scientist', 'Company': 'DataCamp', 'Duration': 'Sep 2017 - May 2020'}, {'Title': 'Curriculum Engineer (Python)', 'Company': 'DataCamp', 'Duration': 'Mar 2016 - May 2020'}, {'Title': 'Postdoctoral Associate/Writer', 'Company': 'Yale University', 'Duration': '2013 - Mar 2016'}], 'Education': [{'Degree': 'Doctor of Philosophy (PhD)', 'Field': 'Pure Mathematics', 'Institution': 'UNSW', 'Duration': '2006 - 2011'}, {'Degree': 'Bachelor of Science (B.S.) with First Class Honors', 'Fields': 'Mathematics, English Literature', 'Institution': 'University of Sydney', 'Duration': '2001 - 2005'}]}\n",
      "Structure test passed!\n"
     ]
    }
   ],
   "source": [
    "def test_structure(parsed_output):\n",
    "    required_fields = [\"Name\", \"Current Role\", \"Location\", \"Previous Roles\", \"Education\"]\n",
    "    assert all(field in parsed_output for field in required_fields), \"Missing required fields\"\n",
    "    assert isinstance(parsed_output[\"Previous Roles\"], list), \"Previous Roles should be a list\"\n",
    "    assert isinstance(parsed_output[\"Education\"], list), \"Education should be a list\"\n",
    "    print(\"Structure test passed!\")\n",
    "\n",
    "# Run the structure test\n",
    "parsed_output = json.loads(output)\n",
    "print(parsed_output)\n",
    "test_structure(parsed_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Batch Processing LinkedIn Profiles\n",
    "\n",
    "After validating JSON mode with one profile, we extend the workflow to process multiple profiles. Text files containing LinkedIn profile data are stored in a `data/` directory and processed in batch. Key steps include:\n",
    "- Reading each text file as input.\n",
    "- Extracting structured data using OpenAI's API.\n",
    "- Validating the JSON output for each profile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where LinkedIn profile files are stored\n",
    "profile_dir = \"data\"\n",
    "\n",
    "# Function to validate JSON\n",
    "def validate_json(raw_output):\n",
    "    try:\n",
    "        parsed_output = json.loads(raw_output)\n",
    "        return parsed_output, True\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Invalid JSON: {e}\")\n",
    "        return None, False\n",
    "\n",
    "# Function to process profiles\n",
    "def process_profiles(directory):\n",
    "    profiles = []\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith(\".txt\"):\n",
    "            with open(os.path.join(directory, file), \"r\") as f:\n",
    "                linkedin_text = f.read()\n",
    "                print(f\"Processing: {file}\")\n",
    "                \n",
    "                # Define the prompt for the LLM\n",
    "                messages = [\n",
    "                    {\"role\": \"user\", \"content\": f\"\"\"\n",
    "                    Extract the following structured information from the text below:\n",
    "                    - Name\n",
    "                    - Current Role\n",
    "                    - Location\n",
    "                    - Previous Roles\n",
    "                    - Education\n",
    "\n",
    "                    Text: {linkedin_text}\n",
    "\n",
    "                    Output the result as a JSON object.\n",
    "                    \"\"\"}\n",
    "                ]\n",
    "\n",
    "                # Make LLM API call\n",
    "                try:\n",
    "                    response = client.chat.completions.create(\n",
    "                        model=\"gpt-4o-mini\",\n",
    "                        response_format={ \"type\": \"json_object\"},\n",
    "                        messages=messages\n",
    "                    )\n",
    "                    raw_output = response.choices[0].message.content\n",
    "\n",
    "                    # Validate JSON\n",
    "                    parsed_output, is_valid = validate_json(raw_output)\n",
    "                    if is_valid:\n",
    "                        profiles.append({\"file_name\": file, \"parsed_output\": parsed_output})\n",
    "                    else:\n",
    "                        print(f\"Skipping invalid JSON for file: {file}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing file {file}: {e}\")\n",
    "    return profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: hamelLI.txt\n",
      "Processing: dagworks.txt\n",
      "Processing: hbaLI.txt\n",
      "Processing: shreyaLI.txt\n",
      "Processing: stefanLI.txt\n",
      "Processing: chipLI.txt\n"
     ]
    }
   ],
   "source": [
    "profiles = process_profiles(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing profile: data/stefanLI.txt\n",
      "\n",
      "Iteration: 1\n",
      "Output for Iteration 1: {'Name': 'Stefan Krawczyk', 'Current Role': 'CEO @ DAGWorks Inc.', 'Location': 'San Francisco, California, United States', 'Previous Roles': ['Co-creator of Hamilton & Burr', 'Pipelines & Agents: Data, Data Science, Machine Learning, & LLMs'], 'Education': []}\n",
      "\n",
      "Iteration: 2\n",
      "Output for Iteration 2: {'Name': 'Stefan Krawczyk', 'Current Role': 'CEO @ DAGWorks Inc.', 'Location': 'San Francisco, California, United States', 'Previous Roles': ['Co-creator of Hamilton & Burr', 'Pipelines & Agents: Data, Data Science, Machine Learning, & LLMs'], 'Education': []}\n",
      "\n",
      "Iteration: 3\n",
      "Output for Iteration 3: {'Name': 'Stefan Krawczyk', 'Current Role': 'CEO @ DAGWorks Inc.', 'Location': 'San Francisco, California, United States', 'Previous Roles': ['Co-creator of Hamilton & Burr', 'Pipelines & Agents: Data, Data Science, Machine Learning, & LLMs'], 'Education': None}\n",
      "\n",
      "Iteration: 4\n",
      "Output for Iteration 4: {'Name': 'Stefan Krawczyk', 'Current Role': 'CEO @ DAGWorks Inc.', 'Location': 'San Francisco, California, United States', 'Previous Roles': ['Co-creator of Hamilton & Burr', 'Pipelines & Agents: Data, Data Science, Machine Learning, & LLMs'], 'Education': None}\n",
      "\n",
      "Iteration: 5\n",
      "Output for Iteration 5: {'Name': 'Stefan Krawczyk', 'Current Role': 'CEO @ DAGWorks Inc.', 'Location': 'San Francisco, California, United States', 'Previous Roles': ['Co-creator of Hamilton & Burr', 'Pipelines & Agents: Data, Data Science, Machine Learning, & LLMs'], 'Education': ''}\n",
      "\n",
      "Iteration: 6\n",
      "Output for Iteration 6: {'Name': 'Stefan Krawczyk', 'Current Role': 'CEO @ DAGWorks Inc.', 'Location': 'San Francisco, California, United States', 'Previous Roles': 'Co-creator of Hamilton & Burr, Pipelines & Agents: Data, Data Science, Machine Learning, & LLMs', 'Education': ''}\n",
      "\n",
      "Iteration: 7\n",
      "Output for Iteration 7: {'Name': 'Stefan Krawczyk', 'Current Role': 'CEO @ DAGWorks Inc.', 'Location': 'San Francisco, California, United States', 'Previous Roles': ['Co-creator of Hamilton & Burr', 'Pipelines & Agents: Data, Data Science, Machine Learning, & LLMs'], 'Education': None}\n",
      "\n",
      "Iteration: 8\n",
      "Output for Iteration 8: {'Name': 'Stefan Krawczyk', 'Current Role': 'CEO @ DAGWorks Inc.', 'Location': 'San Francisco, California, United States', 'Previous Roles': ['Co-creator of Hamilton & Burr', 'Pipelines & Agents: Data, Data Science, Machine Learning, & LLMs'], 'Education': None}\n",
      "\n",
      "Iteration: 9\n",
      "Output for Iteration 9: {'Name': 'Stefan Krawczyk', 'Current Role': 'CEO @ DAGWorks Inc.', 'Location': 'San Francisco, California, United States', 'Previous Roles': ['Co-creator of Hamilton & Burr'], 'Education': []}\n",
      "\n",
      "Iteration: 10\n",
      "Output for Iteration 10: {'Name': 'Stefan Krawczyk', 'Current Role': 'CEO @ DAGWorks Inc. | Co-creator of Hamilton & Burr | Pipelines & Agents: Data, Data Science, Machine Learning, & LLMs', 'Location': 'San Francisco, California, United States', 'Previous Roles': [], 'Education': []}\n",
      "\n",
      "Iteration: 11\n",
      "Output for Iteration 11: {'Name': 'Stefan Krawczyk', 'Current Role': 'CEO @ DAGWorks Inc.', 'Location': 'San Francisco, California, United States', 'Previous Roles': ['Co-creator of Hamilton & Burr', 'Pipelines & Agents: Data, Data Science, Machine Learning, & LLMs'], 'Education': None}\n",
      "\n",
      "Iteration: 12\n",
      "Output for Iteration 12: {'Name': 'Stefan Krawczyk', 'Current Role': 'CEO @ DAGWorks Inc.', 'Location': 'San Francisco, California, United States', 'Previous Roles': ['Co-creator of Hamilton & Burr'], 'Education': []}\n",
      "\n",
      "Iteration: 13\n",
      "Output for Iteration 13: {'Name': 'Stefan Krawczyk', 'Current Role': 'CEO @ DAGWorks Inc.', 'Location': 'San Francisco, California, United States', 'Previous Roles': ['Co-creator of Hamilton & Burr', 'Pipelines & Agents: Data, Data Science, Machine Learning, & LLMs'], 'Education': None}\n",
      "\n",
      "Iteration: 14\n",
      "Output for Iteration 14: {'Name': 'Stefan Krawczyk', 'Current Role': 'CEO @ DAGWorks Inc.', 'Location': 'San Francisco, California, United States', 'Previous Roles': ['Co-creator of Hamilton & Burr', 'Pipelines & Agents: Data, Data Science, Machine Learning, & LLMs'], 'Education': []}\n",
      "\n",
      "Iteration: 15\n",
      "Output for Iteration 15: {'Name': 'Stefan Krawczyk', 'Current Role': 'CEO @ DAGWorks Inc. | Co-creator of Hamilton & Burr | Pipelines & Agents: Data, Data Science, Machine Learning, & LLMs', 'Location': 'San Francisco, California, United States', 'Previous Roles': [], 'Education': []}\n",
      "\n",
      "Iteration: 16\n",
      "Output for Iteration 16: {'Name': 'Stefan Krawczyk', 'Current Role': 'CEO @ DAGWorks Inc.', 'Location': 'San Francisco, California, United States', 'Previous Roles': ['Co-creator of Hamilton & Burr', 'Pipelines & Agents: Data, Data Science, Machine Learning, & LLMs'], 'Education': None}\n",
      "\n",
      "Iteration: 17\n",
      "Output for Iteration 17: {'Name': 'Stefan Krawczyk', 'Current Role': 'CEO @ DAGWorks Inc.', 'Location': 'San Francisco, California, United States', 'Previous Roles': ['Co-creator of Hamilton & Burr', 'Pipelines & Agents: Data, Data Science, Machine Learning, & LLMs'], 'Education': None}\n",
      "\n",
      "Iteration: 18\n",
      "Output for Iteration 18: {'Name': 'Stefan Krawczyk', 'Current Role': 'CEO @ DAGWorks Inc.', 'Location': 'San Francisco, California, United States', 'Previous Roles': ['Co-creator of Hamilton & Burr', 'Pipelines & Agents: Data, Data Science, Machine Learning, & LLMs'], 'Education': []}\n",
      "\n",
      "Iteration: 19\n",
      "Output for Iteration 19: {'Name': 'Stefan Krawczyk', 'Current Role': 'CEO @ DAGWorks Inc.', 'Location': 'San Francisco, California, United States', 'Previous Roles': ['Co-creator of Hamilton & Burr', 'Pipelines & Agents: Data, Data Science, Machine Learning, & LLMs'], 'Education': None}\n",
      "\n",
      "Iteration: 20\n",
      "Output for Iteration 20: {'Name': 'Stefan Krawczyk', 'Current Role': 'CEO @ DAGWorks Inc. | Co-creator of Hamilton & Burr | Pipelines & Agents: Data, Data Science, Machine Learning, & LLMs', 'Location': 'San Francisco, California, United States', 'Previous Roles': [], 'Education': []}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'iteration': 1,\n",
       "  'parsed_output': {'Name': 'Stefan Krawczyk',\n",
       "   'Current Role': 'CEO @ DAGWorks Inc.',\n",
       "   'Location': 'San Francisco, California, United States',\n",
       "   'Previous Roles': ['Co-creator of Hamilton & Burr',\n",
       "    'Pipelines & Agents: Data, Data Science, Machine Learning, & LLMs'],\n",
       "   'Education': []}},\n",
       " {'iteration': 2,\n",
       "  'parsed_output': {'Name': 'Stefan Krawczyk',\n",
       "   'Current Role': 'CEO @ DAGWorks Inc.',\n",
       "   'Location': 'San Francisco, California, United States',\n",
       "   'Previous Roles': ['Co-creator of Hamilton & Burr',\n",
       "    'Pipelines & Agents: Data, Data Science, Machine Learning, & LLMs'],\n",
       "   'Education': []}},\n",
       " {'iteration': 3,\n",
       "  'parsed_output': {'Name': 'Stefan Krawczyk',\n",
       "   'Current Role': 'CEO @ DAGWorks Inc.',\n",
       "   'Location': 'San Francisco, California, United States',\n",
       "   'Previous Roles': ['Co-creator of Hamilton & Burr',\n",
       "    'Pipelines & Agents: Data, Data Science, Machine Learning, & LLMs'],\n",
       "   'Education': None}},\n",
       " {'iteration': 4,\n",
       "  'parsed_output': {'Name': 'Stefan Krawczyk',\n",
       "   'Current Role': 'CEO @ DAGWorks Inc.',\n",
       "   'Location': 'San Francisco, California, United States',\n",
       "   'Previous Roles': ['Co-creator of Hamilton & Burr',\n",
       "    'Pipelines & Agents: Data, Data Science, Machine Learning, & LLMs'],\n",
       "   'Education': None}},\n",
       " {'iteration': 5,\n",
       "  'parsed_output': {'Name': 'Stefan Krawczyk',\n",
       "   'Current Role': 'CEO @ DAGWorks Inc.',\n",
       "   'Location': 'San Francisco, California, United States',\n",
       "   'Previous Roles': ['Co-creator of Hamilton & Burr',\n",
       "    'Pipelines & Agents: Data, Data Science, Machine Learning, & LLMs'],\n",
       "   'Education': ''}},\n",
       " {'iteration': 6,\n",
       "  'parsed_output': {'Name': 'Stefan Krawczyk',\n",
       "   'Current Role': 'CEO @ DAGWorks Inc.',\n",
       "   'Location': 'San Francisco, California, United States',\n",
       "   'Previous Roles': 'Co-creator of Hamilton & Burr, Pipelines & Agents: Data, Data Science, Machine Learning, & LLMs',\n",
       "   'Education': ''}},\n",
       " {'iteration': 7,\n",
       "  'parsed_output': {'Name': 'Stefan Krawczyk',\n",
       "   'Current Role': 'CEO @ DAGWorks Inc.',\n",
       "   'Location': 'San Francisco, California, United States',\n",
       "   'Previous Roles': ['Co-creator of Hamilton & Burr',\n",
       "    'Pipelines & Agents: Data, Data Science, Machine Learning, & LLMs'],\n",
       "   'Education': None}},\n",
       " {'iteration': 8,\n",
       "  'parsed_output': {'Name': 'Stefan Krawczyk',\n",
       "   'Current Role': 'CEO @ DAGWorks Inc.',\n",
       "   'Location': 'San Francisco, California, United States',\n",
       "   'Previous Roles': ['Co-creator of Hamilton & Burr',\n",
       "    'Pipelines & Agents: Data, Data Science, Machine Learning, & LLMs'],\n",
       "   'Education': None}},\n",
       " {'iteration': 9,\n",
       "  'parsed_output': {'Name': 'Stefan Krawczyk',\n",
       "   'Current Role': 'CEO @ DAGWorks Inc.',\n",
       "   'Location': 'San Francisco, California, United States',\n",
       "   'Previous Roles': ['Co-creator of Hamilton & Burr'],\n",
       "   'Education': []}},\n",
       " {'iteration': 10,\n",
       "  'parsed_output': {'Name': 'Stefan Krawczyk',\n",
       "   'Current Role': 'CEO @ DAGWorks Inc. | Co-creator of Hamilton & Burr | Pipelines & Agents: Data, Data Science, Machine Learning, & LLMs',\n",
       "   'Location': 'San Francisco, California, United States',\n",
       "   'Previous Roles': [],\n",
       "   'Education': []}},\n",
       " {'iteration': 11,\n",
       "  'parsed_output': {'Name': 'Stefan Krawczyk',\n",
       "   'Current Role': 'CEO @ DAGWorks Inc.',\n",
       "   'Location': 'San Francisco, California, United States',\n",
       "   'Previous Roles': ['Co-creator of Hamilton & Burr',\n",
       "    'Pipelines & Agents: Data, Data Science, Machine Learning, & LLMs'],\n",
       "   'Education': None}},\n",
       " {'iteration': 12,\n",
       "  'parsed_output': {'Name': 'Stefan Krawczyk',\n",
       "   'Current Role': 'CEO @ DAGWorks Inc.',\n",
       "   'Location': 'San Francisco, California, United States',\n",
       "   'Previous Roles': ['Co-creator of Hamilton & Burr'],\n",
       "   'Education': []}},\n",
       " {'iteration': 13,\n",
       "  'parsed_output': {'Name': 'Stefan Krawczyk',\n",
       "   'Current Role': 'CEO @ DAGWorks Inc.',\n",
       "   'Location': 'San Francisco, California, United States',\n",
       "   'Previous Roles': ['Co-creator of Hamilton & Burr',\n",
       "    'Pipelines & Agents: Data, Data Science, Machine Learning, & LLMs'],\n",
       "   'Education': None}},\n",
       " {'iteration': 14,\n",
       "  'parsed_output': {'Name': 'Stefan Krawczyk',\n",
       "   'Current Role': 'CEO @ DAGWorks Inc.',\n",
       "   'Location': 'San Francisco, California, United States',\n",
       "   'Previous Roles': ['Co-creator of Hamilton & Burr',\n",
       "    'Pipelines & Agents: Data, Data Science, Machine Learning, & LLMs'],\n",
       "   'Education': []}},\n",
       " {'iteration': 15,\n",
       "  'parsed_output': {'Name': 'Stefan Krawczyk',\n",
       "   'Current Role': 'CEO @ DAGWorks Inc. | Co-creator of Hamilton & Burr | Pipelines & Agents: Data, Data Science, Machine Learning, & LLMs',\n",
       "   'Location': 'San Francisco, California, United States',\n",
       "   'Previous Roles': [],\n",
       "   'Education': []}},\n",
       " {'iteration': 16,\n",
       "  'parsed_output': {'Name': 'Stefan Krawczyk',\n",
       "   'Current Role': 'CEO @ DAGWorks Inc.',\n",
       "   'Location': 'San Francisco, California, United States',\n",
       "   'Previous Roles': ['Co-creator of Hamilton & Burr',\n",
       "    'Pipelines & Agents: Data, Data Science, Machine Learning, & LLMs'],\n",
       "   'Education': None}},\n",
       " {'iteration': 17,\n",
       "  'parsed_output': {'Name': 'Stefan Krawczyk',\n",
       "   'Current Role': 'CEO @ DAGWorks Inc.',\n",
       "   'Location': 'San Francisco, California, United States',\n",
       "   'Previous Roles': ['Co-creator of Hamilton & Burr',\n",
       "    'Pipelines & Agents: Data, Data Science, Machine Learning, & LLMs'],\n",
       "   'Education': None}},\n",
       " {'iteration': 18,\n",
       "  'parsed_output': {'Name': 'Stefan Krawczyk',\n",
       "   'Current Role': 'CEO @ DAGWorks Inc.',\n",
       "   'Location': 'San Francisco, California, United States',\n",
       "   'Previous Roles': ['Co-creator of Hamilton & Burr',\n",
       "    'Pipelines & Agents: Data, Data Science, Machine Learning, & LLMs'],\n",
       "   'Education': []}},\n",
       " {'iteration': 19,\n",
       "  'parsed_output': {'Name': 'Stefan Krawczyk',\n",
       "   'Current Role': 'CEO @ DAGWorks Inc.',\n",
       "   'Location': 'San Francisco, California, United States',\n",
       "   'Previous Roles': ['Co-creator of Hamilton & Burr',\n",
       "    'Pipelines & Agents: Data, Data Science, Machine Learning, & LLMs'],\n",
       "   'Education': None}},\n",
       " {'iteration': 20,\n",
       "  'parsed_output': {'Name': 'Stefan Krawczyk',\n",
       "   'Current Role': 'CEO @ DAGWorks Inc. | Co-creator of Hamilton & Burr | Pipelines & Agents: Data, Data Science, Machine Learning, & LLMs',\n",
       "   'Location': 'San Francisco, California, United States',\n",
       "   'Previous Roles': [],\n",
       "   'Education': []}}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Function to validate JSON output (you should define this or use your existing version)\n",
    "def validate_json(json_string):\n",
    "    try:\n",
    "        import json\n",
    "        parsed = json.loads(json_string)\n",
    "        return parsed, True\n",
    "    except json.JSONDecodeError:\n",
    "        return None, False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to process a single profile multiple times\n",
    "def process_single_profile(file_path, iterations=20, print_after_iteration=True):\n",
    "    outputs = []\n",
    "    \n",
    "    with open(file_path, \"r\") as f:\n",
    "        linkedin_text = f.read()\n",
    "        print(f\"Processing profile: {file_path}\")\n",
    "        \n",
    "        for i in range(iterations):\n",
    "            print(f\"\\nIteration: {i + 1}\")\n",
    "            \n",
    "            # Define the prompt for the LLM\n",
    "            messages = [\n",
    "                {\"role\": \"user\", \"content\": f\"\"\"\n",
    "                Extract the following structured information from the text below:\n",
    "                - Name\n",
    "                - Current Role\n",
    "                - Location\n",
    "                - Previous Roles\n",
    "                - Education\n",
    "\n",
    "                Text: {linkedin_text}\n",
    "\n",
    "                Output the result as a JSON object.\n",
    "                \"\"\"}\n",
    "            ]\n",
    "\n",
    "            # Make LLM API call\n",
    "            try:\n",
    "                response = client.chat.completions.create(\n",
    "                    model=\"gpt-4o-mini\",\n",
    "                    response_format={\"type\": \"json_object\"},\n",
    "                    messages=messages\n",
    "                )\n",
    "                raw_output = response.choices[0].message.content\n",
    "                \n",
    "                # Validate JSON\n",
    "                parsed_output, is_valid = validate_json(raw_output)\n",
    "                if is_valid:\n",
    "                    outputs.append({\"iteration\": i + 1, \"parsed_output\": parsed_output})\n",
    "                    if print_after_iteration:\n",
    "                        print(f\"Output for Iteration {i + 1}: {parsed_output}\")\n",
    "                else:\n",
    "                    print(f\"Invalid JSON output at iteration {i + 1}. Skipping.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error on iteration {i + 1}: {e}\")\n",
    "\n",
    "    return outputs\n",
    "\n",
    "# Example usage\n",
    "file_path='data/stefanLI.txt'\n",
    "process_single_profile(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Validating JSON Output\n",
    "\n",
    "Each JSON output from the LLM is validated to ensure:\n",
    "- The JSON is syntactically valid (using `json.loads()`).\n",
    "- All required fields are present (`Name`, `Current Role`, `Location`, `Previous Roles`, `Education`).\n",
    "- The data types of fields match expectations:\n",
    "  - `Previous Roles` and `Education` should be lists.\n",
    "  - Other fields should be strings.\n",
    "\n",
    "Profiles failing validation are flagged for review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to verify fields and structure\n",
    "def verify_fields(json_obj):\n",
    "    required_fields = [\"Name\", \"Current Role\", \"Location\", \"Previous Roles\", \"Education\"]\n",
    "    \n",
    "    # Check for required fields\n",
    "    missing_fields = [field for field in required_fields if field not in json_obj]\n",
    "    if missing_fields:\n",
    "        print(f\"❌ Missing fields: {missing_fields}\")\n",
    "        return False\n",
    "    else:\n",
    "        print(f\"✅ All required fields are present\")\n",
    "    \n",
    "    # Check data types for specific fields\n",
    "    if not isinstance(json_obj[\"Previous Roles\"], list):\n",
    "        print(\"❌ 'Previous Roles' should be a list\")\n",
    "        return False\n",
    "    if not isinstance(json_obj[\"Education\"], list):\n",
    "        print(\"❌ 'Education' should be a list\")\n",
    "        return False\n",
    "    \n",
    "    # All checks passed\n",
    "    print(\"✅ Structure is valid\")\n",
    "    return True\n",
    "\n",
    "# Apply field verification to profiles\n",
    "valid_profiles = []\n",
    "invalid_profiles = []\n",
    "\n",
    "for profile in profiles:\n",
    "    file_name = profile[\"file_name\"]\n",
    "    json_obj = profile[\"parsed_output\"]\n",
    "    print(f\"📝 Verifying: {file_name}\")\n",
    "    \n",
    "    if verify_fields(json_obj):\n",
    "        valid_profiles.append(profile)\n",
    "        print(f\"✅ Profile from {file_name} is valid\")\n",
    "    else:\n",
    "        print(f\"❌ Invalid JSON structure in: {file_name}\")\n",
    "        invalid_profiles.append(profile)\n",
    "\n",
    "# Log the results\n",
    "print(f\"✨ Valid profiles: {len(valid_profiles)}\")\n",
    "print(f\"🚨 Invalid profiles: {len(invalid_profiles)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiles[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Adding Flags to Suspicious Profiles\n",
    "\n",
    "Profiles with missing or unexpected fields are flagged for domain expert review. Examples of flags include:\n",
    "- Missing fields like `Previous Roles` or `Education`.\n",
    "- Incorrectly formatted fields.\n",
    "\n",
    "Flags help identify potential issues in profile extraction and validation logic. Below are examples of flagged profiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flag_suspicious_profiles(profile):\n",
    "    parsed_output = profile[\"parsed_output\"]\n",
    "    flags = []\n",
    "\n",
    "    # Check for empty important fields\n",
    "    if not parsed_output.get(\"Previous Roles\"):\n",
    "        flags.append(\"Empty 'Previous Roles'\")\n",
    "    if not parsed_output.get(\"Education\"):\n",
    "        flags.append(\"Empty 'Education'\")\n",
    "    \n",
    "    # Heuristic for mismatch: Current Role but no Previous Roles/Education\n",
    "    if parsed_output.get(\"Current Role\") and not parsed_output.get(\"Previous Roles\") and not parsed_output.get(\"Education\"):\n",
    "        flags.append(\"No career or educational history provided\")\n",
    "    \n",
    "    # Additional checks (e.g., raw text clues)\n",
    "    raw_text = profile.get(\"raw_text\", \"\")\n",
    "    if \"mission\" in raw_text.lower() or \"headquarters\" in raw_text.lower():\n",
    "        flags.append(\"Profile might be a company, not a person\")\n",
    "    \n",
    "    return flags\n",
    "\n",
    "# Apply the flagging to profiles\n",
    "for profile in profiles:\n",
    "    file_name = profile[\"file_name\"]\n",
    "    flags = flag_suspicious_profiles(profile)\n",
    "    if flags:\n",
    "        print(f\"🚩 Flags for {file_name}: {', '.join(flags)}\")\n",
    "    else:\n",
    "        print(f\"✅ No flags for {file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Saving Data to CSV\n",
    "\n",
    "Profiles are saved to a CSV file for further analysis and review. The CSV includes:\n",
    "- Profile name.\n",
    "- Extracted fields (e.g., `Name`, `Current Role`).\n",
    "- Flags for validation issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "\n",
    "\n",
    "# Function to save JSON data to CSV\n",
    "def save_json_to_csv(profiles, output_file=\"profiles.csv\"):\n",
    "    # Collect all unique keys from the JSON objects to form the CSV headers\n",
    "    all_keys = set()\n",
    "    for profile in profiles:\n",
    "        parsed_output = profile[\"parsed_output\"]\n",
    "        all_keys.update(parsed_output.keys())\n",
    "\n",
    "    # Convert the set of keys into a sorted list to ensure consistent ordering in the CSV\n",
    "    headers = sorted(list(all_keys))\n",
    "    headers.insert(0, \"file_name\")  # Add file_name as the first column\n",
    "    headers.append(\"flags\")  # Add flags as the last column\n",
    "\n",
    "    with open(output_file, mode=\"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=headers)\n",
    "        writer.writeheader()\n",
    "\n",
    "        for profile in profiles:\n",
    "            # Start with the file_name and flags\n",
    "            row = {\"file_name\": profile[\"file_name\"], \"flags\": \", \".join(flag_suspicious_profiles(profile))}\n",
    "            # Add the parsed JSON fields\n",
    "            parsed_output = profile[\"parsed_output\"]\n",
    "            for key in headers:\n",
    "                if key in parsed_output:\n",
    "                    value = parsed_output[key]\n",
    "                    # Convert lists or dictionaries into strings\n",
    "                    if isinstance(value, list):\n",
    "                        row[key] = \"; \".join([str(item) for item in value])\n",
    "                    elif isinstance(value, dict):\n",
    "                        row[key] = str(value)\n",
    "                    else:\n",
    "                        row[key] = value\n",
    "                elif key not in [\"file_name\", \"flags\"]:  # Avoid overwriting file_name or flags\n",
    "                    row[key] = \"\"  # Leave empty if the field does not exist in the JSON\n",
    "            writer.writerow(row)\n",
    "\n",
    "    print(f\"🚀 Profiles saved to {output_file}\")\n",
    "\n",
    "\n",
    "# Save to CSV\n",
    "save_json_to_csv(profiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Displaying Data as Editable Table\n",
    "\n",
    "The CSV data is rendered as an interactive table where:\n",
    "- Flags and extracted fields can be manually reviewed.\n",
    "- Annotations (e.g., comments, notes) can be added for each profile.\n",
    "\n",
    "Below is the editable table for reviewing the profiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install dash dash-table pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dash\n",
    "from dash import Dash, dash_table, html, Input, Output, ctx\n",
    "import pandas as pd\n",
    "\n",
    "# Load your CSV into a DataFrame\n",
    "csv_file_path = \"profiles.csv\"\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Create a Dash app\n",
    "app = Dash(__name__)\n",
    "\n",
    "# Layout of the app\n",
    "app.layout = html.Div([\n",
    "    html.H1(\"Editable Profiles Table\"),\n",
    "    dash_table.DataTable(\n",
    "        id='editable-table',\n",
    "        columns=[{\"name\": col, \"id\": col, \"editable\": True} for col in df.columns],\n",
    "        data=df.to_dict('records'),\n",
    "        editable=True,\n",
    "        row_deletable=True,\n",
    "        style_table={'overflowX': 'auto'},\n",
    "        style_cell={'textAlign': 'left', 'padding': '5px'},\n",
    "    ),\n",
    "    html.Button(\"Save Changes\", id='save-button', n_clicks=0),\n",
    "    html.Div(id='save-output', style={\"margin-top\": \"20px\"})\n",
    "])\n",
    "\n",
    "# Callback to save changes back to CSV\n",
    "@app.callback(\n",
    "    Output('save-output', 'children'),\n",
    "    Input('save-button', 'n_clicks'),\n",
    "    Input('editable-table', 'data')\n",
    ")\n",
    "def save_to_csv(n_clicks, rows):\n",
    "    if \"save-button\" == ctx.triggered_id:  # Ensure save button was clicked\n",
    "        edited_df = pd.DataFrame(rows)\n",
    "        edited_df.to_csv(csv_file_path, index=False)\n",
    "        return \"✅ Changes saved to 'profiles_with_flags.csv'\"\n",
    "    return \"\"\n",
    "\n",
    "# Run the app\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Adding Unit Tests for Profile Validation with Pytest\n",
    "\n",
    "At this point, we’ve validated and flagged the profiles, and it’s time to implement unit tests using pytest to automate the validation process and ensure the robustness of our workflow. These tests will check that:\n",
    "- The JSON output is syntactically correct.\n",
    "- The required fields are present.\n",
    "- The data types of the fields are correct.\n",
    "\n",
    "### Test Setup\n",
    "\n",
    "To integrate pytest, you will need to install the pytest package first.\n",
    "\n",
    "### Writing Tests with Pytest\n",
    "\n",
    "We’ll create a test file that includes the following tests:\n",
    "- Test for valid JSON format: Ensures that the JSON output is correctly parsed.\n",
    "- Test for required fields: Validates that the required fields (Name, Current Role, Location, Previous Roles, Education) are present in the profile.\n",
    "- Test for data types: Ensures that fields like Previous Roles and Education are lists.\n",
    "- Test for missing required field: Flags missing required fields.\n",
    "- Test for invalid data type: Flags invalid data types for fields that should be lists.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ipytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipytest\n",
    "import pytest\n",
    "import sys\n",
    "\n",
    "# Clean up old test functions\n",
    "test_funcs = [name for name in dir(sys.modules[__name__]) if name.startswith('test_')]\n",
    "for func in test_funcs:\n",
    "    delattr(sys.modules[__name__], func)\n",
    "\n",
    "def test_profiles_structure():\n",
    "    \"\"\"Test that all profiles have the required keys\"\"\"\n",
    "    required_fields = [\"Name\", \"Current Role\", \"Location\", \"Previous Roles\", \"Education\"]\n",
    "    \n",
    "    for idx, profile in enumerate(profiles):\n",
    "        # Check top level structure\n",
    "        assert isinstance(profile, dict), f\"Profile {idx} should be a dictionary\"\n",
    "        assert \"file_name\" in profile, f\"Profile {idx} missing file_name\"\n",
    "        assert \"parsed_output\" in profile, f\"Profile {idx} missing parsed_output\"\n",
    "        \n",
    "        # Check parsed_output has all required fields\n",
    "        parsed = profile[\"parsed_output\"]\n",
    "        for field in required_fields:\n",
    "            assert field in parsed, f\"Profile {idx} missing {field} in parsed_output\"\n",
    "\n",
    "ipytest.run('-v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                # Define the prompt for the LLM\n",
    "                messages = [\n",
    "                    {\"role\": \"user\", \"content\": f\"\"\"\n",
    "                    Extract the following structured information from the text below:\n",
    "                    - Name\n",
    "                    - Current Role\n",
    "                    - Location\n",
    "                    - Previous Roles\n",
    "                    - Education\n",
    "\n",
    "                    Text: {linkedin_text}\n",
    "\n",
    "                    Output the result as a JSON object.\n",
    "                    \"\"\"}\n",
    "                ]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "email-assistant-8_QxSGx7-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
