{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LinkedIn Profile Extraction and Validation Workflow\n",
    "\n",
    "This notebook walks through a workflow for extracting structured data from LinkedIn profiles using an LLM, validating the outputs, and preparing the data for domain expert review. The key steps include:\n",
    "1. Extracting structured JSON data from unstructured LinkedIn text.\n",
    "2. Validating the JSON output.\n",
    "3. Flagging suspicious profiles based on missing or invalid fields.\n",
    "4. Saving the data to a CSV file.\n",
    "5. Reviewing and annotating the profiles in an interactive table.\n",
    "\n",
    "This process can be applied to LinkedIn profiles for individuals or companies to assist with data validation, annotation, and review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Testing the Pipeline with One Profile\n",
    "\n",
    "Before applying the process to all profiles, we start by testing the pipeline with a single LinkedIn profile. This ensures that:\n",
    "- The LLM generates structured JSON data.\n",
    "- The JSON output is valid and complete.\n",
    "\n",
    "We'll use one text file (`<profile_name>.txt`) as input and observe the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the file content into a string\n",
    "file_path = \"data/hbaLI.txt\"  # Replace with your actual file path\n",
    "\n",
    "# Open the file in read mode and read the content\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    linkedin_text = file.read()\n",
    "\n",
    "# Print the content to verify\n",
    "# print(linkedin_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /home/codespace/.cache/pypoetry/virtualenvs/email-assistant-8_QxSGx7-py3.12/lib/python3.12/site-packages (1.52.2)\n",
      "Collecting openai\n",
      "  Downloading openai-1.57.0-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/codespace/.cache/pypoetry/virtualenvs/email-assistant-8_QxSGx7-py3.12/lib/python3.12/site-packages (from openai) (4.6.2.post1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/codespace/.cache/pypoetry/virtualenvs/email-assistant-8_QxSGx7-py3.12/lib/python3.12/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/codespace/.cache/pypoetry/virtualenvs/email-assistant-8_QxSGx7-py3.12/lib/python3.12/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /home/codespace/.cache/pypoetry/virtualenvs/email-assistant-8_QxSGx7-py3.12/lib/python3.12/site-packages (from openai) (0.6.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /home/codespace/.cache/pypoetry/virtualenvs/email-assistant-8_QxSGx7-py3.12/lib/python3.12/site-packages (from openai) (2.9.2)\n",
      "Requirement already satisfied: sniffio in /home/codespace/.cache/pypoetry/virtualenvs/email-assistant-8_QxSGx7-py3.12/lib/python3.12/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /home/codespace/.cache/pypoetry/virtualenvs/email-assistant-8_QxSGx7-py3.12/lib/python3.12/site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /home/codespace/.cache/pypoetry/virtualenvs/email-assistant-8_QxSGx7-py3.12/lib/python3.12/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /home/codespace/.cache/pypoetry/virtualenvs/email-assistant-8_QxSGx7-py3.12/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /home/codespace/.cache/pypoetry/virtualenvs/email-assistant-8_QxSGx7-py3.12/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /home/codespace/.cache/pypoetry/virtualenvs/email-assistant-8_QxSGx7-py3.12/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (1.0.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/codespace/.cache/pypoetry/virtualenvs/email-assistant-8_QxSGx7-py3.12/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/codespace/.cache/pypoetry/virtualenvs/email-assistant-8_QxSGx7-py3.12/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /home/codespace/.cache/pypoetry/virtualenvs/email-assistant-8_QxSGx7-py3.12/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
      "Downloading openai-1.57.0-py3-none-any.whl (389 kB)\n",
      "Installing collected packages: openai\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 1.52.2\n",
      "    Uninstalling openai-1.52.2:\n",
      "      Successfully uninstalled openai-1.52.2\n",
      "Successfully installed openai-1.57.0\n"
     ]
    }
   ],
   "source": [
    "! pip install --upgrade openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Client.__init__() got an unexpected keyword argument 'proxies'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 7\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[1;32m      5\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msk-proj--mpTncjCqkBMVhrRFzt-Yruc35myQrOhKV3caYhn7ZyZVFMFz2AkkK_kUvrz0cCJQw2CIrGJg9T3BlbkFJ3t8U3ydwu2c_hWqcT0gmDSSsP0nCgGOndTHBdCQDkORZE_Ga7cS_3gcvKccHeJgmXVRpZYF0cA\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 7\u001b[0m client \u001b[38;5;241m=\u001b[39m \u001b[43mOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Define the user message (chat-style format)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     12\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124mExtract the following structured information from the text below:\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m}\n\u001b[1;32m     24\u001b[0m ]\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/email-assistant-8_QxSGx7-py3.12/lib/python3.12/site-packages/openai/_client.py:123\u001b[0m, in \u001b[0;36mOpenAI.__init__\u001b[0;34m(self, api_key, organization, project, base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m base_url \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    121\u001b[0m     base_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://api.openai.com/v1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 123\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mversion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m__version__\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhttp_client\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhttp_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_strict_response_validation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_strict_response_validation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_default_stream_cls \u001b[38;5;241m=\u001b[39m Stream\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompletions \u001b[38;5;241m=\u001b[39m resources\u001b[38;5;241m.\u001b[39mCompletions(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/email-assistant-8_QxSGx7-py3.12/lib/python3.12/site-packages/openai/_base_client.py:856\u001b[0m, in \u001b[0;36mSyncAPIClient.__init__\u001b[0;34m(self, version, base_url, max_retries, timeout, transport, proxies, limits, http_client, custom_headers, custom_query, _strict_response_validation)\u001b[0m\n\u001b[1;32m    839\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    840\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid `http_client` argument; Expected an instance of `httpx.Client` but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(http_client)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    841\u001b[0m     )\n\u001b[1;32m    843\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    844\u001b[0m     version\u001b[38;5;241m=\u001b[39mversion,\n\u001b[1;32m    845\u001b[0m     limits\u001b[38;5;241m=\u001b[39mlimits,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    854\u001b[0m     _strict_response_validation\u001b[38;5;241m=\u001b[39m_strict_response_validation,\n\u001b[1;32m    855\u001b[0m )\n\u001b[0;32m--> 856\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client \u001b[38;5;241m=\u001b[39m http_client \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mSyncHttpxClientWrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# cast to a valid type because mypy doesn't understand our type narrowing\u001b[39;49;00m\n\u001b[1;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransport\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlimits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/email-assistant-8_QxSGx7-py3.12/lib/python3.12/site-packages/openai/_base_client.py:754\u001b[0m, in \u001b[0;36m_DefaultHttpxClient.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    752\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlimits\u001b[39m\u001b[38;5;124m\"\u001b[39m, DEFAULT_CONNECTION_LIMITS)\n\u001b[1;32m    753\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfollow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 754\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: Client.__init__() got an unexpected keyword argument 'proxies'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "## os.environ[\"OPENAI_API_KEY\"] = \n",
    "client = OpenAI()\n",
    "\n",
    "\n",
    "# Define the user message (chat-style format)\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": f\"\"\"\n",
    "Extract the following structured information from the text below:\n",
    "- Name\n",
    "- Current Role\n",
    "- Location\n",
    "- Previous Roles\n",
    "- Education\n",
    "\n",
    "Text: {linkedin_text}\n",
    "\n",
    "Output the result as a JSON object.\n",
    "\"\"\"}\n",
    "]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=messages\n",
    ")\n",
    "output = response.choices[0].message.content\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid JSON: Expecting value: line 1 column 1 (char 0)\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Test the output\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[43mtest_valid_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[54], line 5\u001b[0m, in \u001b[0;36mtest_valid_json\u001b[0;34m(output)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtest_valid_json\u001b[39m(output):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 5\u001b[0m         parsed_output \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(parsed_output, \u001b[38;5;28mdict\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput is not a valid JSON object\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValid JSON!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/json/decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscan_once(s, idx)\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def test_valid_json(output):\n",
    "    try:\n",
    "        parsed_output = json.loads(output)\n",
    "        assert isinstance(parsed_output, dict), \"Output is not a valid JSON object\"\n",
    "        print(\"Valid JSON!\")\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Invalid JSON: {e}\")\n",
    "        raise\n",
    "\n",
    "# Test the output\n",
    "test_valid_json(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Switching to OpenAI JSON Mode\n",
    "\n",
    "Using OpenAI's JSON mode ensures that the output is structured as valid JSON by design. This step improved reliability and reduced the need for custom validation logic. We:\n",
    "- Re-ran the API call with `response_format={\"type\": \"json_object\"}`.\n",
    "- Validated that the output adheres to the required JSON structure.\n",
    "\n",
    "Below is the implementation of JSON mode with a single test profile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    response_format={ \"type\": \"json_object\"},\n",
    "    messages=messages\n",
    ")\n",
    "output = response.choices[0].message.content\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the output\n",
    "test_valid_json(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_structure(parsed_output):\n",
    "    required_fields = [\"Name\", \"Current Role\", \"Location\", \"Previous Roles\", \"Education\"]\n",
    "    assert all(field in parsed_output for field in required_fields), \"Missing required fields\"\n",
    "    assert isinstance(parsed_output[\"Previous Roles\"], list), \"Previous Roles should be a list\"\n",
    "    assert isinstance(parsed_output[\"Education\"], list), \"Education should be a list\"\n",
    "    print(\"Structure test passed!\")\n",
    "\n",
    "# Run the structure test\n",
    "parsed_output = json.loads(output)\n",
    "print(parsed_output)\n",
    "test_structure(parsed_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Batch Processing LinkedIn Profiles\n",
    "\n",
    "After validating JSON mode with one profile, we extend the workflow to process multiple profiles. Text files containing LinkedIn profile data are stored in a `data/` directory and processed in batch. Key steps include:\n",
    "- Reading each text file as input.\n",
    "- Extracting structured data using OpenAI's API.\n",
    "- Validating the JSON output for each profile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where LinkedIn profile files are stored\n",
    "profile_dir = \"data\"\n",
    "\n",
    "# Function to validate JSON\n",
    "def validate_json(raw_output):\n",
    "    try:\n",
    "        parsed_output = json.loads(raw_output)\n",
    "        return parsed_output, True\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Invalid JSON: {e}\")\n",
    "        return None, False\n",
    "\n",
    "# Function to process profiles\n",
    "def process_profiles(directory):\n",
    "    profiles = []\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith(\".txt\"):\n",
    "            with open(os.path.join(directory, file), \"r\") as f:\n",
    "                linkedin_text = f.read()\n",
    "                print(f\"Processing: {file}\")\n",
    "                \n",
    "                # Define the prompt for the LLM\n",
    "                messages = [\n",
    "                    {\"role\": \"user\", \"content\": f\"\"\"\n",
    "                    Extract the following structured information from the text below:\n",
    "                    - Name\n",
    "                    - Current Role\n",
    "                    - Location\n",
    "                    - Previous Roles\n",
    "                    - Education\n",
    "\n",
    "                    Text: {linkedin_text}\n",
    "\n",
    "                    Output the result as a JSON object.\n",
    "                    \"\"\"}\n",
    "                ]\n",
    "\n",
    "                # Make LLM API call\n",
    "                try:\n",
    "                    response = client.chat.completions.create(\n",
    "                        model=\"gpt-4o-mini\",\n",
    "                        response_format={ \"type\": \"json_object\"},\n",
    "                        messages=messages\n",
    "                    )\n",
    "                    raw_output = response.choices[0].message.content\n",
    "\n",
    "                    # Validate JSON\n",
    "                    parsed_output, is_valid = validate_json(raw_output)\n",
    "                    if is_valid:\n",
    "                        profiles.append({\"file_name\": file, \"parsed_output\": parsed_output})\n",
    "                    else:\n",
    "                        print(f\"Skipping invalid JSON for file: {file}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing file {file}: {e}\")\n",
    "    return profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiles = process_profiles(\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Validating JSON Output\n",
    "\n",
    "Each JSON output from the LLM is validated to ensure:\n",
    "- The JSON is syntactically valid (using `json.loads()`).\n",
    "- All required fields are present (`Name`, `Current Role`, `Location`, `Previous Roles`, `Education`).\n",
    "- The data types of fields match expectations:\n",
    "  - `Previous Roles` and `Education` should be lists.\n",
    "  - Other fields should be strings.\n",
    "\n",
    "Profiles failing validation are flagged for review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to verify fields and structure\n",
    "def verify_fields(json_obj):\n",
    "    required_fields = [\"Name\", \"Current Role\", \"Location\", \"Previous Roles\", \"Education\"]\n",
    "    \n",
    "    # Check for required fields\n",
    "    missing_fields = [field for field in required_fields if field not in json_obj]\n",
    "    if missing_fields:\n",
    "        print(f\"‚ùå Missing fields: {missing_fields}\")\n",
    "        return False\n",
    "    else:\n",
    "        print(f\"‚úÖ All required fields are present\")\n",
    "    \n",
    "    # Check data types for specific fields\n",
    "    if not isinstance(json_obj[\"Previous Roles\"], list):\n",
    "        print(\"‚ùå 'Previous Roles' should be a list\")\n",
    "        return False\n",
    "    if not isinstance(json_obj[\"Education\"], list):\n",
    "        print(\"‚ùå 'Education' should be a list\")\n",
    "        return False\n",
    "    \n",
    "    # All checks passed\n",
    "    print(\"‚úÖ Structure is valid\")\n",
    "    return True\n",
    "\n",
    "# Apply field verification to profiles\n",
    "valid_profiles = []\n",
    "invalid_profiles = []\n",
    "\n",
    "for profile in profiles:\n",
    "    file_name = profile[\"file_name\"]\n",
    "    json_obj = profile[\"parsed_output\"]\n",
    "    print(f\"üìù Verifying: {file_name}\")\n",
    "    \n",
    "    if verify_fields(json_obj):\n",
    "        valid_profiles.append(profile)\n",
    "        print(f\"‚úÖ Profile from {file_name} is valid\")\n",
    "    else:\n",
    "        print(f\"‚ùå Invalid JSON structure in: {file_name}\")\n",
    "        invalid_profiles.append(profile)\n",
    "\n",
    "# Log the results\n",
    "print(f\"‚ú® Valid profiles: {len(valid_profiles)}\")\n",
    "print(f\"üö® Invalid profiles: {len(invalid_profiles)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiles[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Adding Flags to Suspicious Profiles\n",
    "\n",
    "Profiles with missing or unexpected fields are flagged for domain expert review. Examples of flags include:\n",
    "- Missing fields like `Previous Roles` or `Education`.\n",
    "- Incorrectly formatted fields.\n",
    "\n",
    "Flags help identify potential issues in profile extraction and validation logic. Below are examples of flagged profiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flag_suspicious_profiles(profile):\n",
    "    parsed_output = profile[\"parsed_output\"]\n",
    "    flags = []\n",
    "\n",
    "    # Check for empty important fields\n",
    "    if not parsed_output.get(\"Previous Roles\"):\n",
    "        flags.append(\"Empty 'Previous Roles'\")\n",
    "    if not parsed_output.get(\"Education\"):\n",
    "        flags.append(\"Empty 'Education'\")\n",
    "    \n",
    "    # Heuristic for mismatch: Current Role but no Previous Roles/Education\n",
    "    if parsed_output.get(\"Current Role\") and not parsed_output.get(\"Previous Roles\") and not parsed_output.get(\"Education\"):\n",
    "        flags.append(\"No career or educational history provided\")\n",
    "    \n",
    "    # Additional checks (e.g., raw text clues)\n",
    "    raw_text = profile.get(\"raw_text\", \"\")\n",
    "    if \"mission\" in raw_text.lower() or \"headquarters\" in raw_text.lower():\n",
    "        flags.append(\"Profile might be a company, not a person\")\n",
    "    \n",
    "    return flags\n",
    "\n",
    "# Apply the flagging to profiles\n",
    "for profile in profiles:\n",
    "    file_name = profile[\"file_name\"]\n",
    "    flags = flag_suspicious_profiles(profile)\n",
    "    if flags:\n",
    "        print(f\"üö© Flags for {file_name}: {', '.join(flags)}\")\n",
    "    else:\n",
    "        print(f\"‚úÖ No flags for {file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Saving Data to CSV\n",
    "\n",
    "Profiles are saved to a CSV file for further analysis and review. The CSV includes:\n",
    "- Profile name.\n",
    "- Extracted fields (e.g., `Name`, `Current Role`).\n",
    "- Flags for validation issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def save_profiles_with_flags_to_csv(profiles, output_file=\"profiles_with_flags.csv\"):\n",
    "    with open(output_file, mode=\"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "        # Define the field names for the CSV (adjust as necessary)\n",
    "        fieldnames = [\"file_name\", \"flags\", \"Name\", \"Current Role\", \"Location\", \"Previous Roles\", \"Education\"]\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "        writer.writeheader()\n",
    "        for profile in profiles:\n",
    "            flags = flag_suspicious_profiles(profile)\n",
    "            parsed_output = profile[\"parsed_output\"]\n",
    "\n",
    "            # Flatten the parsed JSON fields for easier CSV writing\n",
    "            writer.writerow({\n",
    "                \"file_name\": profile[\"file_name\"],\n",
    "                \"flags\": \", \".join(flags),\n",
    "                \"Name\": parsed_output.get(\"Name\", \"\"),\n",
    "                \"Current Role\": parsed_output.get(\"Current Role\", \"\"),\n",
    "                \"Location\": parsed_output.get(\"Location\", \"\"),\n",
    "                \"Previous Roles\": \"; \".join([f\"{role.get('Title', '')} at {role.get('Company', '')}\" for role in parsed_output.get(\"Previous Roles\", [])]),\n",
    "                \"Education\": \"; \".join([f\"{edu.get('Degree', '')} in {edu.get('Field', '')} from {edu.get('Institution', '')}\" for edu in parsed_output.get(\"Education\", [])]),\n",
    "            })\n",
    "\n",
    "# Save all profiles with flags to a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def save_json_to_csv(profiles, output_file=\"profiles.csv\"):\n",
    "    # Collect all unique keys from the JSON objects to form the CSV headers\n",
    "    all_keys = set()\n",
    "    for profile in profiles:\n",
    "        parsed_output = profile[\"parsed_output\"]\n",
    "        all_keys.update(parsed_output.keys())\n",
    "\n",
    "    # Convert the set of keys into a sorted list to ensure consistent ordering in the CSV\n",
    "    headers = sorted(list(all_keys))\n",
    "    headers.insert(0, \"file_name\")  # Add file_name as the first column\n",
    "    headers.append(\"flags\")  # Add flags as the last column\n",
    "\n",
    "    with open(output_file, mode=\"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=headers)\n",
    "        writer.writeheader()\n",
    "\n",
    "        for profile in profiles:\n",
    "            # Start with the file_name and flags\n",
    "            row = {\"file_name\": profile[\"file_name\"], \"flags\": \", \".join(flag_suspicious_profiles(profile))}\n",
    "            # Add the parsed JSON fields\n",
    "            parsed_output = profile[\"parsed_output\"]\n",
    "            for key in headers:\n",
    "                if key in parsed_output:\n",
    "                    value = parsed_output[key]\n",
    "                    # Convert lists or dictionaries into strings\n",
    "                    if isinstance(value, list):\n",
    "                        row[key] = \"; \".join([str(item) for item in value])\n",
    "                    elif isinstance(value, dict):\n",
    "                        row[key] = str(value)\n",
    "                    else:\n",
    "                        row[key] = value\n",
    "                elif key not in [\"file_name\", \"flags\"]:  # Avoid overwriting file_name or flags\n",
    "                    row[key] = \"\"  # Leave empty if the field does not exist in the JSON\n",
    "            writer.writerow(row)\n",
    "\n",
    "    print(f\"üöÄ Profiles saved to {output_file}\")\n",
    "\n",
    "# Save to CSV\n",
    "save_json_to_csv(profiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Path to your CSV file\n",
    "csv_file_path = \"profiles_with_flags.csv\"\n",
    "\n",
    "# Load the CSV into a DataFrame\n",
    "try:\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "\n",
    "    # Display the DataFrame as a scrollable table\n",
    "    display(HTML(df.to_html(index=False, notebook=True)))\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå The file {csv_file_path} was not found. Please check the path.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Displaying Data as Editable Table\n",
    "\n",
    "The CSV data is rendered as an interactive table where:\n",
    "- Flags and extracted fields can be manually reviewed.\n",
    "- Annotations (e.g., comments, notes) can be added for each profile.\n",
    "\n",
    "Below is the editable table for reviewing the profiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install dash dash-table pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:8050/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f701491ba70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import dash\n",
    "from dash import Dash, dash_table, html, Input, Output, ctx\n",
    "import pandas as pd\n",
    "\n",
    "# Load your CSV into a DataFrame\n",
    "csv_file_path = \"profiles_with_flags.csv\"\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Create a Dash app\n",
    "app = Dash(__name__)\n",
    "\n",
    "# Layout of the app\n",
    "app.layout = html.Div([\n",
    "    html.H1(\"Editable Profiles Table\"),\n",
    "    dash_table.DataTable(\n",
    "        id='editable-table',\n",
    "        columns=[{\"name\": col, \"id\": col, \"editable\": True} for col in df.columns],\n",
    "        data=df.to_dict('records'),\n",
    "        editable=True,\n",
    "        row_deletable=True,\n",
    "        style_table={'overflowX': 'auto'},\n",
    "        style_cell={'textAlign': 'left', 'padding': '5px'},\n",
    "    ),\n",
    "    html.Button(\"Save Changes\", id='save-button', n_clicks=0),\n",
    "    html.Div(id='save-output', style={\"margin-top\": \"20px\"})\n",
    "])\n",
    "\n",
    "# Callback to save changes back to CSV\n",
    "@app.callback(\n",
    "    Output('save-output', 'children'),\n",
    "    Input('save-button', 'n_clicks'),\n",
    "    Input('editable-table', 'data')\n",
    ")\n",
    "def save_to_csv(n_clicks, rows):\n",
    "    if \"save-button\" == ctx.triggered_id:  # Ensure save button was clicked\n",
    "        edited_df = pd.DataFrame(rows)\n",
    "        edited_df.to_csv(csv_file_path, index=False)\n",
    "        return \"‚úÖ Changes saved to 'profiles_with_flags.csv'\"\n",
    "    return \"\"\n",
    "\n",
    "# Run the app\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "email-assistant-8_QxSGx7-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
