{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LinkedIn Profile Extraction and Validation Workflow\n",
    "\n",
    "This notebook walks through a workflow for extracting structured data from LinkedIn profiles using an LLM, validating the outputs, and preparing the data for domain expert review. The key steps include:\n",
    "1. Extracting structured JSON data from unstructured LinkedIn text.\n",
    "2. Validating the JSON output.\n",
    "3. Flagging suspicious profiles based on missing or invalid fields.\n",
    "4. Saving the data to a CSV file.\n",
    "5. Reviewing and annotating the profiles in an interactive table.\n",
    "\n",
    "This process can be applied to LinkedIn profiles for individuals or companies to assist with data validation, annotation, and review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Testing the Pipeline with One Profile\n",
    "\n",
    "Before applying the process to all profiles, we start by testing the pipeline with a single LinkedIn profile. This ensures that:\n",
    "- The LLM generates structured JSON data.\n",
    "- The JSON output is valid and complete.\n",
    "\n",
    "We'll use one text file (`<profile_name>.txt`) as input and observe the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the file content into a string\n",
    "file_path = \"data/hbaLI.txt\"  # Replace with your actual file path\n",
    "\n",
    "# Open the file in read mode and read the content\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    linkedin_text = file.read()\n",
    "\n",
    "# Print the content to verify\n",
    "# print(linkedin_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"Name\": \"Hugo Bowne-Anderson\",\n",
      "  \"Current Role\": \"Independent Data and AI Scientist, Consultant, Writer, Educator, Podcaster\",\n",
      "  \"Location\": \"Darlinghurst, New South Wales, Australia\",\n",
      "  \"Previous Roles\": [\n",
      "    {\n",
      "      \"Role\": \"Head of Developer Relations\",\n",
      "      \"Company\": \"Outerbounds\",\n",
      "      \"Duration\": \"Feb 2022 - Aug 2024\"\n",
      "    },\n",
      "    {\n",
      "      \"Role\": \"Head of Data Science Evangelism and Marketing\",\n",
      "      \"Company\": \"Coiled\",\n",
      "      \"Duration\": \"May 2020 - Oct 2021\"\n",
      "    },\n",
      "    {\n",
      "      \"Role\": \"Data Scientist\",\n",
      "      \"Company\": \"DataCamp\",\n",
      "      \"Duration\": \"Sep 2017 - May 2020\"\n",
      "    },\n",
      "    {\n",
      "      \"Role\": \"Curriculum Engineer (Python)\",\n",
      "      \"Company\": \"DataCamp\",\n",
      "      \"Duration\": \"Mar 2016 - May 2020\"\n",
      "    },\n",
      "    {\n",
      "      \"Role\": \"Postdoctoral Associate/Writer\",\n",
      "      \"Company\": \"Yale University\",\n",
      "      \"Duration\": \"2013 - Mar 2016\"\n",
      "    }\n",
      "  ],\n",
      "  \"Education\": [\n",
      "    {\n",
      "      \"Institution\": \"UNSW\",\n",
      "      \"Degree\": \"Doctor of Philosophy (PhD)\",\n",
      "      \"Field\": \"Pure Mathematics\",\n",
      "      \"Duration\": \"2006 - 2011\"\n",
      "    },\n",
      "    {\n",
      "      \"Institution\": \"University of Sydney\",\n",
      "      \"Degree\": \"Bachelor of Science (B.S.) (First Class Honors)\",\n",
      "      \"Field\": \"Mathematics, English Literature\",\n",
      "      \"Duration\": \"2001 - 2005\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = 'XXX'\n",
    "client = OpenAI()\n",
    "\n",
    "\n",
    "# Define the user message (chat-style format)\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": f\"\"\"\n",
    "Extract the following structured information from the text below:\n",
    "- Name\n",
    "- Current Role\n",
    "- Location\n",
    "- Previous Roles\n",
    "- Education\n",
    "\n",
    "Text: {linkedin_text}\n",
    "\n",
    "Output the result as a JSON object.\n",
    "\"\"\"}\n",
    "]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=messages\n",
    ")\n",
    "output = response.choices[0].message.content\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid JSON: Expecting value: line 1 column 1 (char 0)\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Test the output\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[43mtest_valid_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m, in \u001b[0;36mtest_valid_json\u001b[0;34m(output)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtest_valid_json\u001b[39m(output):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 5\u001b[0m         parsed_output \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(parsed_output, \u001b[38;5;28mdict\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput is not a valid JSON object\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValid JSON!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/json/decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscan_once(s, idx)\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def test_valid_json(output):\n",
    "    try:\n",
    "        parsed_output = json.loads(output)\n",
    "        assert isinstance(parsed_output, dict), \"Output is not a valid JSON object\"\n",
    "        print(\"Valid JSON!\")\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Invalid JSON: {e}\")\n",
    "        raise\n",
    "\n",
    "# Test the output\n",
    "test_valid_json(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Switching to OpenAI JSON Mode\n",
    "\n",
    "Using OpenAI's JSON mode ensures that the output is structured as valid JSON by design. This step improved reliability and reduced the need for custom validation logic. We:\n",
    "- Re-ran the API call with `response_format={\"type\": \"json_object\"}`.\n",
    "- Validated that the output adheres to the required JSON structure.\n",
    "\n",
    "Below is the implementation of JSON mode with a single test profile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"Name\": \"Hugo Bowne-Anderson\",\n",
      "  \"Current Role\": \"Independent Data and AI Scientist, Consultant, Writer, Educator, Podcaster\",\n",
      "  \"Location\": \"Darlinghurst, New South Wales, Australia\",\n",
      "  \"Previous Roles\": [\n",
      "    {\n",
      "      \"Title\": \"Head of Developer Relations\",\n",
      "      \"Company\": \"Outerbounds\",\n",
      "      \"Duration\": \"Feb 2022 - Aug 2024\"\n",
      "    },\n",
      "    {\n",
      "      \"Title\": \"Head of Data Science Evangelism and Marketing\",\n",
      "      \"Company\": \"Coiled\",\n",
      "      \"Duration\": \"May 2020 - Oct 2021\"\n",
      "    },\n",
      "    {\n",
      "      \"Title\": \"Data Scientist\",\n",
      "      \"Company\": \"DataCamp\",\n",
      "      \"Duration\": \"Sep 2017 - May 2020\"\n",
      "    },\n",
      "    {\n",
      "      \"Title\": \"Curriculum Engineer (Python)\",\n",
      "      \"Company\": \"DataCamp\",\n",
      "      \"Duration\": \"Mar 2016 - May 2020\"\n",
      "    },\n",
      "    {\n",
      "      \"Title\": \"Postdoctoral Associate/Writer\",\n",
      "      \"Company\": \"Yale University\",\n",
      "      \"Duration\": \"2013 - Mar 2016\"\n",
      "    }\n",
      "  ],\n",
      "  \"Education\": [\n",
      "    {\n",
      "      \"Degree\": \"Doctor of Philosophy (PhD)\",\n",
      "      \"Field\": \"Pure Mathematics\",\n",
      "      \"Institution\": \"UNSW\",\n",
      "      \"Duration\": \"2006 - 2011\"\n",
      "    },\n",
      "    {\n",
      "      \"Degree\": \"Bachelor of Science (B.S.) (First Class Honors)\",\n",
      "      \"Field\": \"Mathematics, English Literature\",\n",
      "      \"Institution\": \"University of Sydney\",\n",
      "      \"Duration\": \"2001 - 2005\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    response_format={ \"type\": \"json_object\"},\n",
    "    messages=messages\n",
    ")\n",
    "output = response.choices[0].message.content\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid JSON!\n"
     ]
    }
   ],
   "source": [
    "# Test the output\n",
    "test_valid_json(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Name': 'Hugo Bowne-Anderson', 'Current Role': 'Independent Data and AI Scientist, Consultant, Writer, Educator, Podcaster', 'Location': 'Darlinghurst, New South Wales, Australia', 'Previous Roles': [{'Title': 'Head of Developer Relations', 'Company': 'Outerbounds', 'Duration': 'Feb 2022 - Aug 2024'}, {'Title': 'Head of Data Science Evangelism and Marketing', 'Company': 'Coiled', 'Duration': 'May 2020 - Oct 2021'}, {'Title': 'Data Scientist', 'Company': 'DataCamp', 'Duration': 'Sep 2017 - May 2020'}, {'Title': 'Curriculum Engineer (Python)', 'Company': 'DataCamp', 'Duration': 'Mar 2016 - May 2020'}, {'Title': 'Postdoctoral Associate/Writer', 'Company': 'Yale University', 'Duration': '2013 - Mar 2016'}], 'Education': [{'Degree': 'Doctor of Philosophy (PhD)', 'Field': 'Pure Mathematics', 'Institution': 'UNSW', 'Duration': '2006 - 2011'}, {'Degree': 'Bachelor of Science (B.S.) (First Class Honors)', 'Field': 'Mathematics, English Literature', 'Institution': 'University of Sydney', 'Duration': '2001 - 2005'}]}\n",
      "Structure test passed!\n"
     ]
    }
   ],
   "source": [
    "def test_structure(parsed_output):\n",
    "    required_fields = [\"Name\", \"Current Role\", \"Location\", \"Previous Roles\", \"Education\"]\n",
    "    assert all(field in parsed_output for field in required_fields), \"Missing required fields\"\n",
    "    assert isinstance(parsed_output[\"Previous Roles\"], list), \"Previous Roles should be a list\"\n",
    "    assert isinstance(parsed_output[\"Education\"], list), \"Education should be a list\"\n",
    "    print(\"Structure test passed!\")\n",
    "\n",
    "# Run the structure test\n",
    "parsed_output = json.loads(output)\n",
    "print(parsed_output)\n",
    "test_structure(parsed_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Batch Processing LinkedIn Profiles\n",
    "\n",
    "After validating JSON mode with one profile, we extend the workflow to process multiple profiles. Text files containing LinkedIn profile data are stored in a `data/` directory and processed in batch. Key steps include:\n",
    "- Reading each text file as input.\n",
    "- Extracting structured data using OpenAI's API.\n",
    "- Validating the JSON output for each profile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where LinkedIn profile files are stored\n",
    "profile_dir = \"data\"\n",
    "\n",
    "# Function to validate JSON\n",
    "def validate_json(raw_output):\n",
    "    try:\n",
    "        parsed_output = json.loads(raw_output)\n",
    "        return parsed_output, True\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Invalid JSON: {e}\")\n",
    "        return None, False\n",
    "\n",
    "# Function to process profiles\n",
    "def process_profiles(directory):\n",
    "    profiles = []\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith(\".txt\"):\n",
    "            with open(os.path.join(directory, file), \"r\") as f:\n",
    "                linkedin_text = f.read()\n",
    "                print(f\"Processing: {file}\")\n",
    "                \n",
    "                # Define the prompt for the LLM\n",
    "                messages = [\n",
    "                    {\"role\": \"user\", \"content\": f\"\"\"\n",
    "                    Extract the following structured information from the text below:\n",
    "                    - Name\n",
    "                    - Current Role\n",
    "                    - Location\n",
    "                    - Previous Roles\n",
    "                    - Education\n",
    "\n",
    "                    Text: {linkedin_text}\n",
    "\n",
    "                    Output the result as a JSON object.\n",
    "                    \"\"\"}\n",
    "                ]\n",
    "\n",
    "                # Make LLM API call\n",
    "                try:\n",
    "                    response = client.chat.completions.create(\n",
    "                        model=\"gpt-4o-mini\",\n",
    "                        response_format={ \"type\": \"json_object\"},\n",
    "                        messages=messages\n",
    "                    )\n",
    "                    raw_output = response.choices[0].message.content\n",
    "\n",
    "                    # Validate JSON\n",
    "                    parsed_output, is_valid = validate_json(raw_output)\n",
    "                    if is_valid:\n",
    "                        profiles.append({\"file_name\": file, \"parsed_output\": parsed_output})\n",
    "                    else:\n",
    "                        print(f\"Skipping invalid JSON for file: {file}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing file {file}: {e}\")\n",
    "    return profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: hamelLI.txt\n",
      "Processing: dagworks.txt\n",
      "Processing: hbaLI.txt\n",
      "Processing: shreyaLI.txt\n",
      "Processing: stefanLI.txt\n",
      "Processing: chipLI.txt\n"
     ]
    }
   ],
   "source": [
    "profiles = process_profiles(\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Validating JSON Output\n",
    "\n",
    "Each JSON output from the LLM is validated to ensure:\n",
    "- The JSON is syntactically valid (using `json.loads()`).\n",
    "- All required fields are present (`Name`, `Current Role`, `Location`, `Previous Roles`, `Education`).\n",
    "- The data types of fields match expectations:\n",
    "  - `Previous Roles` and `Education` should be lists.\n",
    "  - Other fields should be strings.\n",
    "\n",
    "Profiles failing validation are flagged for review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Verifying: hamelLI.txt\n",
      "✅ All required fields are present\n",
      "✅ Structure is valid\n",
      "✅ Profile from hamelLI.txt is valid\n",
      "📝 Verifying: dagworks.txt\n",
      "✅ All required fields are present\n",
      "✅ Structure is valid\n",
      "✅ Profile from dagworks.txt is valid\n",
      "📝 Verifying: hbaLI.txt\n",
      "✅ All required fields are present\n",
      "✅ Structure is valid\n",
      "✅ Profile from hbaLI.txt is valid\n",
      "📝 Verifying: shreyaLI.txt\n",
      "✅ All required fields are present\n",
      "✅ Structure is valid\n",
      "✅ Profile from shreyaLI.txt is valid\n",
      "📝 Verifying: stefanLI.txt\n",
      "✅ All required fields are present\n",
      "✅ Structure is valid\n",
      "✅ Profile from stefanLI.txt is valid\n",
      "📝 Verifying: chipLI.txt\n",
      "✅ All required fields are present\n",
      "✅ Structure is valid\n",
      "✅ Profile from chipLI.txt is valid\n",
      "✨ Valid profiles: 6\n",
      "🚨 Invalid profiles: 0\n"
     ]
    }
   ],
   "source": [
    "# Function to verify fields and structure\n",
    "def verify_fields(json_obj):\n",
    "    required_fields = [\"Name\", \"Current Role\", \"Location\", \"Previous Roles\", \"Education\"]\n",
    "    \n",
    "    # Check for required fields\n",
    "    missing_fields = [field for field in required_fields if field not in json_obj]\n",
    "    if missing_fields:\n",
    "        print(f\"❌ Missing fields: {missing_fields}\")\n",
    "        return False\n",
    "    else:\n",
    "        print(f\"✅ All required fields are present\")\n",
    "    \n",
    "    # Check data types for specific fields\n",
    "    if not isinstance(json_obj[\"Previous Roles\"], list):\n",
    "        print(\"❌ 'Previous Roles' should be a list\")\n",
    "        return False\n",
    "    if not isinstance(json_obj[\"Education\"], list):\n",
    "        print(\"❌ 'Education' should be a list\")\n",
    "        return False\n",
    "    \n",
    "    # All checks passed\n",
    "    print(\"✅ Structure is valid\")\n",
    "    return True\n",
    "\n",
    "# Apply field verification to profiles\n",
    "valid_profiles = []\n",
    "invalid_profiles = []\n",
    "\n",
    "for profile in profiles:\n",
    "    file_name = profile[\"file_name\"]\n",
    "    json_obj = profile[\"parsed_output\"]\n",
    "    print(f\"📝 Verifying: {file_name}\")\n",
    "    \n",
    "    if verify_fields(json_obj):\n",
    "        valid_profiles.append(profile)\n",
    "        print(f\"✅ Profile from {file_name} is valid\")\n",
    "    else:\n",
    "        print(f\"❌ Invalid JSON structure in: {file_name}\")\n",
    "        invalid_profiles.append(profile)\n",
    "\n",
    "# Log the results\n",
    "print(f\"✨ Valid profiles: {len(valid_profiles)}\")\n",
    "print(f\"🚨 Invalid profiles: {len(invalid_profiles)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'file_name': 'hamelLI.txt',\n",
       "  'parsed_output': {'Name': 'Hamel Husain',\n",
       "   'Current Role': 'Founder @ Parlance Labs | Entrepreneur in Residence, ML & Data Science',\n",
       "   'Location': 'United States',\n",
       "   'Previous Roles': [{'Role': 'Scout',\n",
       "     'Company': 'Bain Capital',\n",
       "     'Duration': 'Aug 2024 - Present'},\n",
       "    {'Role': 'Entrepreneur in Residence',\n",
       "     'Company': 'fast.ai',\n",
       "     'Duration': 'Aug 2022 - May 2023'},\n",
       "    {'Role': 'Core Contributor & Maintainer',\n",
       "     'Company': 'fast.ai',\n",
       "     'Duration': 'Sep 2019 - Aug 2022'},\n",
       "    {'Role': 'Head of ML & Data Science',\n",
       "     'Company': 'Outerbounds',\n",
       "     'Duration': 'Jan 2022 - Aug 2022'},\n",
       "    {'Role': 'Staff Machine Learning Engineer',\n",
       "     'Company': 'GitHub',\n",
       "     'Duration': 'Oct 2017 - Jan 2022'}],\n",
       "   'Education': [{'Degree': 'Master of Science (M.S.)',\n",
       "     'Field': 'Computer Science, Machine Learning',\n",
       "     'Institution': 'Georgia Institute of Technology'},\n",
       "    {'Degree': 'Doctor of Law (J.D.), Cum Laude',\n",
       "     'Institution': 'University of Michigan',\n",
       "     'Notes': 'Graduated, but I decided to not pursue a career in law and instead follow my passion for data science and machine learning.'}]}},\n",
       " {'file_name': 'dagworks.txt',\n",
       "  'parsed_output': {'Name': 'Thierry Jean',\n",
       "   'Current Role': 'Founding Engineer',\n",
       "   'Location': 'San Francisco, California',\n",
       "   'Previous Roles': [],\n",
       "   'Education': []}},\n",
       " {'file_name': 'hbaLI.txt',\n",
       "  'parsed_output': {'Name': 'Hugo Bowne-Anderson',\n",
       "   'Current Role': 'Independent Data and AI Scientist, Consultant, Writer, Educator, Podcaster',\n",
       "   'Location': 'Darlinghurst, New South Wales, Australia',\n",
       "   'Previous Roles': [{'Role': 'Head of Developer Relations',\n",
       "     'Company': 'Outerbounds',\n",
       "     'Duration': 'Feb 2022 - Aug 2024'},\n",
       "    {'Role': 'Head of Data Science Evangelism and Marketing',\n",
       "     'Company': 'Coiled',\n",
       "     'Duration': 'May 2020 - Oct 2021'},\n",
       "    {'Role': 'Data Scientist',\n",
       "     'Company': 'DataCamp',\n",
       "     'Duration': 'Sep 2017 - May 2020'},\n",
       "    {'Role': 'Curriculum Engineer (Python)',\n",
       "     'Company': 'DataCamp',\n",
       "     'Duration': 'Mar 2016 - May 2020'},\n",
       "    {'Role': 'Postdoctoral Associate/Writer',\n",
       "     'Company': 'Yale University',\n",
       "     'Duration': '2013 - Mar 2016'}],\n",
       "   'Education': [{'Degree': 'Doctor of Philosophy (PhD)',\n",
       "     'Field': 'Pure Mathematics',\n",
       "     'Institution': 'UNSW',\n",
       "     'Duration': '2006 - 2011'},\n",
       "    {'Degree': 'Bachelor of Science (B.S.) (First Class Honors)',\n",
       "     'Field': 'Mathematics, English Literature',\n",
       "     'Institution': 'University of Sydney',\n",
       "     'Duration': '2001 - 2005'}]}},\n",
       " {'file_name': 'shreyaLI.txt',\n",
       "  'parsed_output': {'Name': 'Shreya Shankar',\n",
       "   'Current Role': 'Machine Learning Engineer at UC Berkeley',\n",
       "   'Location': 'Berkeley, California, United States',\n",
       "   'Previous Roles': [{'Role': '1st ML engineer',\n",
       "     'Company': 'Viaduct',\n",
       "     'Duration': 'Jun 2019 - Jan 2021'},\n",
       "    {'Role': 'ML research',\n",
       "     'Company': 'Google Brain',\n",
       "     'Duration': 'Sep 2017 - Apr 2019'},\n",
       "    {'Role': 'Co-Director',\n",
       "     'Company': 'she++',\n",
       "     'Duration': 'Sep 2015 - Jan 2018'},\n",
       "    {'Role': 'Software Engineering Intern',\n",
       "     'Company': 'Facebook',\n",
       "     'Duration': 'Jun 2017 - Sep 2017'},\n",
       "    {'Role': 'CS106 Section Leader/Teaching Assistant',\n",
       "     'Company': 'Stanford University',\n",
       "     'Duration': 'Jan 2016 - Jun 2017'}],\n",
       "   'Education': [{'Institution': 'UC Berkeley',\n",
       "     'Degree': 'Doctor of Philosophy - PhD',\n",
       "     'Field': 'Computer Science',\n",
       "     'Year': '2021'},\n",
       "    {'Institution': 'Stanford University',\n",
       "     'Degree': \"Master's degree\",\n",
       "     'Field': 'Computer Science',\n",
       "     'Year': '2018 - 2019'}]}},\n",
       " {'file_name': 'stefanLI.txt',\n",
       "  'parsed_output': {'Name': 'Stefan Krawczyk',\n",
       "   'Current Role': 'CEO @ DAGWorks Inc. | Co-creator of Hamilton & Burr | Pipelines & Agents: Data, Data Science, Machine Learning, & LLMs',\n",
       "   'Location': 'San Francisco, California, United States',\n",
       "   'Previous Roles': [],\n",
       "   'Education': []}},\n",
       " {'file_name': 'chipLI.txt',\n",
       "  'parsed_output': {'Name': 'Chip Huyen',\n",
       "   'Current Role': 'Designing ML Systems, Teaching Machine Learning Systems Design at Stanford',\n",
       "   'Location': 'San Francisco, California, United States',\n",
       "   'Previous Roles': ['VP of AI & OSS at Voltron Data',\n",
       "    'Co-Founder at Claypot AI',\n",
       "    'Adjunct Lecturer at Stanford University',\n",
       "    'ML Engineer & Open Source Lead at Snorkel AI',\n",
       "    'NVIDIA',\n",
       "    'Netflix',\n",
       "    'Primer',\n",
       "    'Baomoi.com (acquired by VNG)'],\n",
       "   'Education': [{'Degree': 'Master’s Degree',\n",
       "     'Field': 'Computer Science',\n",
       "     'Institution': 'Stanford University'},\n",
       "    {'Degree': \"Bachelor's Degree\",\n",
       "     'Field': 'Computer Science',\n",
       "     'Institution': 'Stanford University',\n",
       "     'Activities': ['Creative Writing Group',\n",
       "      'CS198 Section Leading Community']}]}}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profiles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file_name': 'dagworks.txt',\n",
       " 'parsed_output': {'Name': 'Thierry Jean',\n",
       "  'Current Role': 'Founding Engineer',\n",
       "  'Location': 'San Francisco, California',\n",
       "  'Previous Roles': [],\n",
       "  'Education': []}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profiles[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Adding Flags to Suspicious Profiles\n",
    "\n",
    "Profiles with missing or unexpected fields are flagged for domain expert review. Examples of flags include:\n",
    "- Missing fields like `Previous Roles` or `Education`.\n",
    "- Incorrectly formatted fields.\n",
    "\n",
    "Flags help identify potential issues in profile extraction and validation logic. Below are examples of flagged profiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ No flags for hamelLI.txt\n",
      "🚩 Flags for dagworks.txt: Empty 'Previous Roles', Empty 'Education', No career or educational history provided\n",
      "✅ No flags for hbaLI.txt\n",
      "✅ No flags for shreyaLI.txt\n",
      "🚩 Flags for stefanLI.txt: Empty 'Previous Roles', Empty 'Education', No career or educational history provided\n",
      "✅ No flags for chipLI.txt\n"
     ]
    }
   ],
   "source": [
    "def flag_suspicious_profiles(profile):\n",
    "    parsed_output = profile[\"parsed_output\"]\n",
    "    flags = []\n",
    "\n",
    "    # Check for empty important fields\n",
    "    if not parsed_output.get(\"Previous Roles\"):\n",
    "        flags.append(\"Empty 'Previous Roles'\")\n",
    "    if not parsed_output.get(\"Education\"):\n",
    "        flags.append(\"Empty 'Education'\")\n",
    "    \n",
    "    # Heuristic for mismatch: Current Role but no Previous Roles/Education\n",
    "    if parsed_output.get(\"Current Role\") and not parsed_output.get(\"Previous Roles\") and not parsed_output.get(\"Education\"):\n",
    "        flags.append(\"No career or educational history provided\")\n",
    "    \n",
    "    # Additional checks (e.g., raw text clues)\n",
    "    raw_text = profile.get(\"raw_text\", \"\")\n",
    "    if \"mission\" in raw_text.lower() or \"headquarters\" in raw_text.lower():\n",
    "        flags.append(\"Profile might be a company, not a person\")\n",
    "    \n",
    "    return flags\n",
    "\n",
    "# Apply the flagging to profiles\n",
    "for profile in profiles:\n",
    "    file_name = profile[\"file_name\"]\n",
    "    flags = flag_suspicious_profiles(profile)\n",
    "    if flags:\n",
    "        print(f\"🚩 Flags for {file_name}: {', '.join(flags)}\")\n",
    "    else:\n",
    "        print(f\"✅ No flags for {file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Saving Data to CSV\n",
    "\n",
    "Profiles are saved to a CSV file for further analysis and review. The CSV includes:\n",
    "- Profile name.\n",
    "- Extracted fields (e.g., `Name`, `Current Role`).\n",
    "- Flags for validation issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Profiles saved to profiles.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "\n",
    "\n",
    "# Function to save JSON data to CSV\n",
    "def save_json_to_csv(profiles, output_file=\"profiles.csv\"):\n",
    "    # Collect all unique keys from the JSON objects to form the CSV headers\n",
    "    all_keys = set()\n",
    "    for profile in profiles:\n",
    "        parsed_output = profile[\"parsed_output\"]\n",
    "        all_keys.update(parsed_output.keys())\n",
    "\n",
    "    # Convert the set of keys into a sorted list to ensure consistent ordering in the CSV\n",
    "    headers = sorted(list(all_keys))\n",
    "    headers.insert(0, \"file_name\")  # Add file_name as the first column\n",
    "    headers.append(\"flags\")  # Add flags as the last column\n",
    "\n",
    "    with open(output_file, mode=\"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=headers)\n",
    "        writer.writeheader()\n",
    "\n",
    "        for profile in profiles:\n",
    "            # Start with the file_name and flags\n",
    "            row = {\"file_name\": profile[\"file_name\"], \"flags\": \", \".join(flag_suspicious_profiles(profile))}\n",
    "            # Add the parsed JSON fields\n",
    "            parsed_output = profile[\"parsed_output\"]\n",
    "            for key in headers:\n",
    "                if key in parsed_output:\n",
    "                    value = parsed_output[key]\n",
    "                    # Convert lists or dictionaries into strings\n",
    "                    if isinstance(value, list):\n",
    "                        row[key] = \"; \".join([str(item) for item in value])\n",
    "                    elif isinstance(value, dict):\n",
    "                        row[key] = str(value)\n",
    "                    else:\n",
    "                        row[key] = value\n",
    "                elif key not in [\"file_name\", \"flags\"]:  # Avoid overwriting file_name or flags\n",
    "                    row[key] = \"\"  # Leave empty if the field does not exist in the JSON\n",
    "            writer.writerow(row)\n",
    "\n",
    "    print(f\"🚀 Profiles saved to {output_file}\")\n",
    "\n",
    "\n",
    "# Save to CSV\n",
    "save_json_to_csv(profiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Displaying Data as Editable Table\n",
    "\n",
    "The CSV data is rendered as an interactive table where:\n",
    "- Flags and extracted fields can be manually reviewed.\n",
    "- Annotations (e.g., comments, notes) can be added for each profile.\n",
    "\n",
    "Below is the editable table for reviewing the profiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dash\n",
      "  Downloading dash-2.18.2-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting dash-table\n",
      "  Downloading dash_table-5.0.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: pandas in /home/codespace/.cache/pypoetry/virtualenvs/email-assistant-8_QxSGx7-py3.12/lib/python3.12/site-packages (2.2.3)\n",
      "Collecting Flask<3.1,>=1.0.4 (from dash)\n",
      "  Downloading flask-3.0.3-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting Werkzeug<3.1 (from dash)\n",
      "  Downloading werkzeug-3.0.6-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting plotly>=5.0.0 (from dash)\n",
      "  Downloading plotly-5.24.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting dash-html-components==2.0.0 (from dash)\n",
      "  Downloading dash_html_components-2.0.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting dash-core-components==2.0.0 (from dash)\n",
      "  Downloading dash_core_components-2.0.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: importlib-metadata in /home/codespace/.cache/pypoetry/virtualenvs/email-assistant-8_QxSGx7-py3.12/lib/python3.12/site-packages (from dash) (8.4.0)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in /home/codespace/.cache/pypoetry/virtualenvs/email-assistant-8_QxSGx7-py3.12/lib/python3.12/site-packages (from dash) (4.12.2)\n",
      "Requirement already satisfied: requests in /home/codespace/.cache/pypoetry/virtualenvs/email-assistant-8_QxSGx7-py3.12/lib/python3.12/site-packages (from dash) (2.32.3)\n",
      "Collecting retrying (from dash)\n",
      "  Downloading retrying-1.3.4-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: nest-asyncio in /home/codespace/.cache/pypoetry/virtualenvs/email-assistant-8_QxSGx7-py3.12/lib/python3.12/site-packages (from dash) (1.6.0)\n",
      "Requirement already satisfied: setuptools in /home/codespace/.cache/pypoetry/virtualenvs/email-assistant-8_QxSGx7-py3.12/lib/python3.12/site-packages (from dash) (75.2.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /home/codespace/.cache/pypoetry/virtualenvs/email-assistant-8_QxSGx7-py3.12/lib/python3.12/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/codespace/.cache/pypoetry/virtualenvs/email-assistant-8_QxSGx7-py3.12/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/codespace/.cache/pypoetry/virtualenvs/email-assistant-8_QxSGx7-py3.12/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/codespace/.cache/pypoetry/virtualenvs/email-assistant-8_QxSGx7-py3.12/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: Jinja2>=3.1.2 in /home/codespace/.cache/pypoetry/virtualenvs/email-assistant-8_QxSGx7-py3.12/lib/python3.12/site-packages (from Flask<3.1,>=1.0.4->dash) (3.1.4)\n",
      "Collecting itsdangerous>=2.1.2 (from Flask<3.1,>=1.0.4->dash)\n",
      "  Downloading itsdangerous-2.2.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: click>=8.1.3 in /home/codespace/.cache/pypoetry/virtualenvs/email-assistant-8_QxSGx7-py3.12/lib/python3.12/site-packages (from Flask<3.1,>=1.0.4->dash) (8.1.7)\n",
      "Collecting blinker>=1.6.2 (from Flask<3.1,>=1.0.4->dash)\n",
      "  Downloading blinker-1.9.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /home/codespace/.cache/pypoetry/virtualenvs/email-assistant-8_QxSGx7-py3.12/lib/python3.12/site-packages (from plotly>=5.0.0->dash) (8.5.0)\n",
      "Requirement already satisfied: packaging in /home/codespace/.cache/pypoetry/virtualenvs/email-assistant-8_QxSGx7-py3.12/lib/python3.12/site-packages (from plotly>=5.0.0->dash) (24.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/codespace/.cache/pypoetry/virtualenvs/email-assistant-8_QxSGx7-py3.12/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/codespace/.cache/pypoetry/virtualenvs/email-assistant-8_QxSGx7-py3.12/lib/python3.12/site-packages (from Werkzeug<3.1->dash) (2.1.5)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/codespace/.cache/pypoetry/virtualenvs/email-assistant-8_QxSGx7-py3.12/lib/python3.12/site-packages (from importlib-metadata->dash) (3.20.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.cache/pypoetry/virtualenvs/email-assistant-8_QxSGx7-py3.12/lib/python3.12/site-packages (from requests->dash) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.cache/pypoetry/virtualenvs/email-assistant-8_QxSGx7-py3.12/lib/python3.12/site-packages (from requests->dash) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.cache/pypoetry/virtualenvs/email-assistant-8_QxSGx7-py3.12/lib/python3.12/site-packages (from requests->dash) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.cache/pypoetry/virtualenvs/email-assistant-8_QxSGx7-py3.12/lib/python3.12/site-packages (from requests->dash) (2024.8.30)\n",
      "Downloading dash-2.18.2-py3-none-any.whl (7.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dash_table-5.0.0-py3-none-any.whl (3.9 kB)\n",
      "Downloading dash_core_components-2.0.0-py3-none-any.whl (3.8 kB)\n",
      "Downloading dash_html_components-2.0.0-py3-none-any.whl (4.1 kB)\n",
      "Downloading flask-3.0.3-py3-none-any.whl (101 kB)\n",
      "Downloading plotly-5.24.1-py3-none-any.whl (19.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.1/19.1 MB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading werkzeug-3.0.6-py3-none-any.whl (227 kB)\n",
      "Downloading retrying-1.3.4-py3-none-any.whl (11 kB)\n",
      "Downloading blinker-1.9.0-py3-none-any.whl (8.5 kB)\n",
      "Downloading itsdangerous-2.2.0-py3-none-any.whl (16 kB)\n",
      "Installing collected packages: dash-table, dash-html-components, dash-core-components, Werkzeug, retrying, plotly, itsdangerous, blinker, Flask, dash\n",
      "Successfully installed Flask-3.0.3 Werkzeug-3.0.6 blinker-1.9.0 dash-2.18.2 dash-core-components-2.0.0 dash-html-components-2.0.0 dash-table-5.0.0 itsdangerous-2.2.0 plotly-5.24.1 retrying-1.3.4\n"
     ]
    }
   ],
   "source": [
    "! pip install dash dash-table pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:8050/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7638ea843ec0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import dash\n",
    "from dash import Dash, dash_table, html, Input, Output, ctx\n",
    "import pandas as pd\n",
    "\n",
    "# Load your CSV into a DataFrame\n",
    "csv_file_path = \"profiles.csv\"\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Create a Dash app\n",
    "app = Dash(__name__)\n",
    "\n",
    "# Layout of the app\n",
    "app.layout = html.Div([\n",
    "    html.H1(\"Editable Profiles Table\"),\n",
    "    dash_table.DataTable(\n",
    "        id='editable-table',\n",
    "        columns=[{\"name\": col, \"id\": col, \"editable\": True} for col in df.columns],\n",
    "        data=df.to_dict('records'),\n",
    "        editable=True,\n",
    "        row_deletable=True,\n",
    "        style_table={'overflowX': 'auto'},\n",
    "        style_cell={'textAlign': 'left', 'padding': '5px'},\n",
    "    ),\n",
    "    html.Button(\"Save Changes\", id='save-button', n_clicks=0),\n",
    "    html.Div(id='save-output', style={\"margin-top\": \"20px\"})\n",
    "])\n",
    "\n",
    "# Callback to save changes back to CSV\n",
    "@app.callback(\n",
    "    Output('save-output', 'children'),\n",
    "    Input('save-button', 'n_clicks'),\n",
    "    Input('editable-table', 'data')\n",
    ")\n",
    "def save_to_csv(n_clicks, rows):\n",
    "    if \"save-button\" == ctx.triggered_id:  # Ensure save button was clicked\n",
    "        edited_df = pd.DataFrame(rows)\n",
    "        edited_df.to_csv(csv_file_path, index=False)\n",
    "        return \"✅ Changes saved to 'profiles_with_flags.csv'\"\n",
    "    return \"\"\n",
    "\n",
    "# Run the app\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:8050/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7638e98be420>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import dash\n",
    "from dash import Dash, dash_table, html, Input, Output, ctx\n",
    "import pandas as pd\n",
    "\n",
    "# Load your CSV into a DataFrame\n",
    "csv_file_path = \"profiles.csv\"\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Add new columns for \"Acceptance\" (checkbox) and \"Notes\"\n",
    "df['Acceptance'] = ['' for _ in range(len(df))]  # Initial empty values for the checkbox column\n",
    "df['Notes'] = ['' for _ in range(len(df))]  # Initial empty values for the notes column\n",
    "\n",
    "# Create a Dash app\n",
    "app = Dash(__name__)\n",
    "\n",
    "# Layout of the app\n",
    "app.layout = html.Div([\n",
    "    html.H1(\"Editable Profiles Table\"),\n",
    "    dash_table.DataTable(\n",
    "        id='editable-table',\n",
    "        columns=[\n",
    "            {\"name\": col, \"id\": col, \"editable\": True} if col not in [\"Acceptance\", \"Notes\"] else {\"name\": col, \"id\": col, \"editable\": True}\n",
    "            for col in df.columns\n",
    "        ],\n",
    "        data=df.to_dict('records'),\n",
    "        editable=True,\n",
    "        row_deletable=True,\n",
    "        style_table={'overflowX': 'auto'},\n",
    "        style_cell={'textAlign': 'left', 'padding': '5px'},\n",
    "    ),\n",
    "    html.Button(\"Save Changes\", id='save-button', n_clicks=0),\n",
    "    html.Div(id='save-output', style={\"margin-top\": \"20px\"})\n",
    "])\n",
    "\n",
    "# Callback to save changes back to CSV\n",
    "@app.callback(\n",
    "    Output('save-output', 'children'),\n",
    "    Input('save-button', 'n_clicks'),\n",
    "    Input('editable-table', 'data')\n",
    ")\n",
    "def save_to_csv(n_clicks, rows):\n",
    "    if \"save-button\" == ctx.triggered_id:  # Ensure save button was clicked\n",
    "        edited_df = pd.DataFrame(rows)\n",
    "        edited_df.to_csv(csv_file_path, index=False)\n",
    "        return \"✅ Changes saved to 'profiles_with_flags.csv'\"\n",
    "    return \"\"\n",
    "\n",
    "# Run the app\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "email-assistant-8_QxSGx7-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
