{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "340fb2fdb3b1d327",
   "metadata": {},
   "source": [
    "# This session's goal:\n",
    "\n",
    "## > Motivation: You need a great SDLC!\n",
    "## > Frameworks: what to know\n",
    "## > Have you build closer to the \"prompt\".\n",
    "\n",
    "## > Things to leave you with:\n",
    "### Idea of the shapes of LLM usage: pipeline vs agentic / agent & human-in-the-loop\n",
    "### A start to an SDLC - by building out an app\n",
    "- testing & eval\n",
    "- micro vs macro\n",
    "\n",
    "# Reinforce:\n",
    "- API calls & prompt engineering\n",
    "- non-determinism -> they're generative models\n",
    "- structured outputs\n",
    "\n",
    "## Things we likely wont get to:\n",
    "- We shipped to prod, now what?\n",
    "- Multi-modal\n",
    "- Retrieval part of RAG\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45380788-2d4f-4259-9d7e-e8652c8b6990",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c36b7f2b-9086-48d4-9fcd-441cd6a4a2e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T19:48:31.859813Z",
     "start_time": "2024-11-07T19:48:31.855372Z"
    }
   },
   "source": [
    "# Motivation\n",
    "\n",
    "[Slides](https://docs.google.com/presentation/d/1sWKF_vyKk6ldMQ1nWX2QexZO9fPMsR9IFD_x82tNVOs/edit?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68976ded69ec4ee5",
   "metadata": {},
   "source": [
    "# Frameworks: what to know"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11822c59-af9e-4ef0-8e4b-5e963cd6a8ea",
   "metadata": {},
   "source": [
    "## 1. Reduce code you need to write\n",
    "See example apps and the code we wrote or didn't write.\n",
    "\n",
    "## 2. But also not slow you down\n",
    "What are you going to need to change and when?\n",
    "\n",
    "### Demo vs Production: they have different needs.\n",
    "  - e.g. yesterday's session -- what is the prompt llama-index was using? how would you optimize it?\n",
    "  - how do you reduce the variance of the outputs?\n",
    "\n",
    "## You only really understand the value of a framework when you understand what it is trying to reduce / provide\n",
    " - hopefully you will leave you with some thoughts here today."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b75b4427d31eeb",
   "metadata": {},
   "source": [
    "# Framework Spectrum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0626f047-85dd-49ae-92d1-a3ebf2dd5307",
   "metadata": {},
   "source": [
    "## 1. Giga libraries / all-in-one:\n",
    "### Opinionated Framework + off-the-shelf implementations / ecosystem that you buy-into. \n",
    "#### E.g. langchain, llama-index, etc.\n",
    "#### Large dependencies\n",
    "\n",
    "## 2. \"Construction\" (glue) frameworks:\n",
    "### Good at help you connect your business logic without getting in the way.\n",
    "### They are neutral and allow you to bring in other tools easily.\n",
    "#### E.g. FastAPI, django, pytorch, Burr, Hamilton\n",
    "#### Small dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceaa9047-1ef8-4e96-b327-a9d513667f96",
   "metadata": {},
   "source": [
    "## 3. Question: which do you want to use when?\n",
    "\n",
    "###  Why is it important to understand what's being sent to the LLM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae654493-a180-4074-875e-fdd34c53ee2a",
   "metadata": {},
   "source": [
    "# Sorry RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ecb6b6f8c42dd",
   "metadata": {},
   "source": [
    "![Simple RAG](img/4-not-rag-focus.png)\n",
    "\n",
    "## We wont do the retrieval part -> just the \"generation\" \n",
    "Retreival is just figuring out the context to add to the LLM. Need to master generation first!\n",
    "\n",
    "> Key part is understanding the LLM calls...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7227641d-61dc-4bfd-84ae-28ad83e5791c",
   "metadata": {},
   "source": [
    "# Next Section\n",
    "\n",
    "This next section will cover:\n",
    "* Building closer to the \"prompt\".\n",
    "* Idea of the shapes of LLM usage: pipeline vs agentic / agent & human-in-the-loop\n",
    "* A start to an SDLC - by building out an app\n",
    "  - testing & eval\n",
    "  - micro vs macro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a3c9d5-7546-43b1-95f0-9a7039f9b7ff",
   "metadata": {},
   "source": [
    "## SDLC we'll begin to stick together:\n",
    "\n",
    "Note 1: no fine-tuning here.\n",
    "\n",
    "Note 2: due to time, we wont dive into automation, or model based & A/B test approaches to evals.\n",
    "\n",
    "![Alt text](img/3-llm-virtuous-cycle.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0100259e-3c5b-41a7-89d7-1c20c1000359",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Recruiting email assistant\n",
    "You're a hiring manager/recruiter and are tasked with cold or warm outreach to get candidates to interview. We're going to slowly build out a little application and try to reinforce some first principles here.\n",
    "\n",
    "If you need our LinkedIn profiles you can find them [here](https://drive.google.com/drive/folders/1enZQYpunRFk2nj8_UVK7iExbtevKf3bH?usp=sharing).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a7126d-7ccb-4e36-8bc5-53303b11fa90",
   "metadata": {},
   "source": [
    "### Refresher on calling openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ed0d24c-6cbb-40bc-a681-48d5b1fccb01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='Why was the math book sad? Because it had too many problems! Mama mia!', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "# set env key if you haven't\n",
    "import os\n",
    "#os.environ['OPENAI_API_KEY'] = .....\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-4o-mini\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are Mario from Super Mario Bros. Answer as Mario, the assistant, only.\"}, \n",
    "    {\"role\": \"user\", \"content\": \"Tell me a joke.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Why did the computer go to the doctor? Because it had a virus! Wahaha!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Tell me another one.\"},  \n",
    "  ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d5d23d-0d7b-4d06-b3d4-8ac8fb25578a",
   "metadata": {},
   "source": [
    "### Quick note on \"prompting techniques\"\n",
    "These are situational -- but parking here in case you need it for the exercise.\n",
    "#### Zero shot\n",
    "No examples.\n",
    "```\n",
    "What are some interesting facts about the Eiffel Tower?\n",
    "```\n",
    "\n",
    "#### One-to N shot prompting\n",
    "In one-shot prompting, you provide one example of the desired output before asking the model to respond. This can help guide the model on the response format or tone.\n",
    "```\n",
    "Provide a synonym for each of these words.  \n",
    "* Happy -> Joyful\n",
    "* Sad -> Downcast  \n",
    "* Excited -> Thrilled  \n",
    "Give a synonym for 'Angry'\n",
    "```\n",
    "\n",
    "#### Chain of Thought (CoT)\n",
    "You show it how to reason/think about something.\n",
    "```\n",
    "If a train leaves the station at 3 PM going 60 miles per hour, how far will it have traveled by 6 PM?  \n",
    "**Example**: '3 PM to 6 PM is 3 hours. 3 hours x 60 miles per hour = 180 miles.'  \n",
    "How far will the train have traveled by 9 PM if it continues at the same speed?\"\n",
    "```\n",
    "\n",
    "#### Add think step by step\n",
    "You add \"think step by step\" to the end of the prompt.\n",
    "```\n",
    "Is the following candidate:\n",
    "{{candidate details}}\n",
    "\n",
    "A match for the following position:\n",
    "{{position details}}\n",
    "Think step by step.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb764aff-f678-4bca-abfe-7c9226336321",
   "metadata": {},
   "source": [
    "### Short overview of cost\n",
    "Generally speaking you pay per \"token\" on inputs and outputs -- for most models they are different rates.\n",
    "\n",
    "#### Token?\n",
    "A token is a something like a \"letter\", \"syllable\", \"symbol\".\n",
    "\n",
    "#### What is my token usage?\n",
    "Most vendor APIs return this value in the response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95e63fe4-95ac-49f7-a8fd-d41ab6607ba8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-ARBDshkedFVwb4HPKvi7M3XrZbrXc',\n",
       " 'choices': [{'finish_reason': 'stop',\n",
       "   'index': 0,\n",
       "   'logprobs': None,\n",
       "   'message': {'content': 'Why was the math book sad? Because it had too many problems! Mama mia!',\n",
       "    'refusal': None,\n",
       "    'role': 'assistant',\n",
       "    'audio': None,\n",
       "    'function_call': None,\n",
       "    'tool_calls': None}}],\n",
       " 'created': 1731041360,\n",
       " 'model': 'gpt-3.5-turbo-0125',\n",
       " 'object': 'chat.completion',\n",
       " 'service_tier': None,\n",
       " 'system_fingerprint': None,\n",
       " 'usage': {'completion_tokens': 17,\n",
       "  'prompt_tokens': 64,\n",
       "  'total_tokens': 81,\n",
       "  'completion_tokens_details': {'audio_tokens': 0,\n",
       "   'reasoning_tokens': 0,\n",
       "   'accepted_prediction_tokens': 0,\n",
       "   'rejected_prediction_tokens': 0},\n",
       "  'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see the \"usage part of the response\"\n",
    "completion.dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cc30af-df5f-4412-98a1-ee154652d31b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### V0. Burr introduction\n",
    "\n",
    "#### What is Burr?\n",
    "It helps you build a \"flowchart\" and then execute (orchestrate) it.\n",
    "\n",
    "> A framework helps with observability and gluing it all together.\n",
    "\n",
    "#### Quick overview of how it works.\n",
    "You write code that:\n",
    "\n",
    "1. Defines \"actions\".\n",
    "2. You wire those actions together to form a graph.\n",
    "3. You make an application out of a graph and run it however you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08878286-b176-4bb7-ae82-cb9a55384815",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 12.0.0 (20240704.0754)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"184pt\" height=\"112pt\"\n",
       " viewBox=\"0.00 0.00 184.10 112.20\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 108.2)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-108.2 180.1,-108.2 180.1,4 -4,4\"/>\n",
       "<!-- increment -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>increment</title>\n",
       "<path fill=\"#b4d8e4\" stroke=\"black\" d=\"M76.35,-104.2C76.35,-104.2 18.75,-104.2 18.75,-104.2 12.75,-104.2 6.75,-98.2 6.75,-92.2 6.75,-92.2 6.75,-79.6 6.75,-79.6 6.75,-73.6 12.75,-67.6 18.75,-67.6 18.75,-67.6 76.35,-67.6 76.35,-67.6 82.35,-67.6 88.35,-73.6 88.35,-79.6 88.35,-79.6 88.35,-92.2 88.35,-92.2 88.35,-98.2 82.35,-104.2 76.35,-104.2\"/>\n",
       "<text text-anchor=\"middle\" x=\"47.55\" y=\"-80.1\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">increment</text>\n",
       "</g>\n",
       "<!-- increment&#45;&gt;increment -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>increment&#45;&gt;increment</title>\n",
       "<path fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" d=\"M88.76,-92.14C98.85,-91.68 106.35,-89.6 106.35,-85.9 106.35,-83.82 103.98,-82.25 100.1,-81.2\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"100.64,-77.74 90.26,-79.86 99.7,-84.67 100.64,-77.74\"/>\n",
       "<text text-anchor=\"middle\" x=\"141.22\" y=\"-80.85\" font-family=\"Times,serif\" font-size=\"14.00\">counter &lt; 10</text>\n",
       "</g>\n",
       "<!-- exit_counter -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>exit_counter</title>\n",
       "<path fill=\"#b4d8e4\" stroke=\"black\" d=\"M83.1,-36.6C83.1,-36.6 12,-36.6 12,-36.6 6,-36.6 0,-30.6 0,-24.6 0,-24.6 0,-12 0,-12 0,-6 6,0 12,0 12,0 83.1,0 83.1,0 89.1,0 95.1,-6 95.1,-12 95.1,-12 95.1,-24.6 95.1,-24.6 95.1,-30.6 89.1,-36.6 83.1,-36.6\"/>\n",
       "<text text-anchor=\"middle\" x=\"47.55\" y=\"-12.5\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">exit_counter</text>\n",
       "</g>\n",
       "<!-- increment&#45;&gt;exit_counter -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>increment&#45;&gt;exit_counter</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M47.55,-67.32C47.55,-61.44 47.55,-54.72 47.55,-48.2\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"51.05,-48.44 47.55,-38.44 44.05,-48.44 51.05,-48.44\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "Graph(actions=[increment: counter -> counter, exit_counter: counter -> {}], transitions=[Transition(from_=increment: counter -> counter, to=increment: counter -> counter, condition=condition: counter < 10), Transition(from_=increment: counter -> counter, to=exit_counter: counter -> {}, condition=condition: default)])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "from burr.core import action, State\n",
    "\n",
    "@action(reads=[\"counter\"], writes=[\"counter\"])\n",
    "def increment(state: State) -> State:\n",
    "    \"\"\"Increment the counter by 1\"\"\"\n",
    "    current_count = state[\"counter\"]  # get the value from the `state`\n",
    "    current_count += 1\n",
    "    print(\"Count: \", current_count)\n",
    "    # use `.update()` to create a new `State`\n",
    "    return state.update(counter=current_count)\n",
    "\n",
    "\n",
    "@action(reads=[\"counter\"], writes=[])\n",
    "def exit_counter(state: State) -> State:\n",
    "    \"\"\"Print the current count and the current time\"\"\"\n",
    "    current_count = state[\"counter\"]\n",
    "    print(f\"Finished counting to {current_count} at {datetime.datetime.now():%H:%M:%S %Y-%m-%d}\")\n",
    "    return state\n",
    "\n",
    "from burr.core import ApplicationBuilder, expr, default\n",
    "from burr.core.graph import GraphBuilder\n",
    "graph = (\n",
    "    GraphBuilder()\n",
    "    .with_actions(increment, exit_counter)\n",
    "    .with_transitions(\n",
    "        (\"increment\", \"increment\", expr(\"counter < 10\")),\n",
    "        (\"increment\", \"exit_counter\", default),\n",
    "    ).build()\n",
    ")\n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "179f21b0-7a4d-4bdd-994b-cd1f3c7c5808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count:  1\n",
      "Count:  2\n",
      "Count:  3\n",
      "Count:  4\n",
      "Count:  5\n",
      "Count:  6\n",
      "Count:  7\n",
      "Count:  8\n",
      "Count:  9\n",
      "Count:  10\n",
      "Finished counting to 10 at 22:56:42 2024-11-07\n"
     ]
    }
   ],
   "source": [
    "# And run it!\n",
    "app = (\n",
    "    ApplicationBuilder()\n",
    "    .with_graph(graph)\n",
    "    .with_state(counter=0)\n",
    "    .with_entrypoint(\"increment\")\n",
    "    .build()\n",
    ")\n",
    "action_obj, result, state = app.run(halt_after=[\"exit_counter\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb065fa3-5e38-4ae9-a760-9eb49ddef8ff",
   "metadata": {},
   "source": [
    "### V1. Given a LinkedIn PDF extract information from it.\n",
    "\n",
    "```bash\n",
    "cd email_assistant/v1\n",
    "```\n",
    "\n",
    "If you just want to play:\n",
    "```bash\n",
    "python app.py\n",
    "```\n",
    "\n",
    "If you want to write some code:\n",
    "> Edit app.py to import `from pipeline import build_pipeline` and comment out importing `pipeline_gold`\n",
    "\n",
    "> Edit pipeline.py\n",
    "\n",
    "```bash\n",
    "python app.py\n",
    "```\n",
    "\n",
    "#### Tasks:\n",
    "- See what you can get the LLM to do\n",
    "- Can you get it to extract information in a structured manner?\n",
    "- What if you tweak the input prompt or data? How consistent is this behavior?\n",
    "- What if you add more and more to be extracted? How well does it perform?\n",
    "\n",
    "#### Recap:\n",
    "\n",
    "1. This is a pipeline\n",
    "2. Wouldn't it be great if we could get better structured output?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1842043d-fc73-460c-9e66-c1bab054b38f",
   "metadata": {},
   "source": [
    "### Structured outputs with OpenAI via Pydantic\n",
    "\n",
    "Output structure isn't guaranteed, but some models now have a way to provide structure that they would enforce.\n",
    "\n",
    "Here we refresh our structured needs, and then show how to use pydantic and openai's new structured output API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "631ac402-55ea-4341-b2f4-b87fab836d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"company\": \"Netflix\",\n",
      "  \"stock_ticker\": \"NFLX\",\n",
      "  \"latest_price\": 123.45\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# Refresher\n",
    "# set env key if you haven't\n",
    "import os\n",
    "#os.environ['OPENAI_API_KEY'] = .....\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "news_wire = \"\"\"\n",
    "Today, Netflix (NFLX) jumped 5% on news of increased subscribers, ending the day at $123.45.\n",
    "\"\"\"\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-4o-mini\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are an excellent financial news information extractor that returns JSON.\"}, \n",
    "    {\"role\": \"user\", \"content\": f\"Extract the company, stock ticker, and latest price from this newswire.\\nNewswire:\\n{news_wire}\"},\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0aa767da-4a64-44d6-9b22-0c38d7691ff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FinancialStockTicker(company='Netflix', stock_ticker='NFLX', latest_price=123.45)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# New API:\n",
    "from pydantic import BaseModel, Field\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "\n",
    "class FinancialStockTicker(BaseModel):\n",
    "    \"\"\"Extract the following financial stock information.\"\"\"\n",
    "    company: str = Field(description=\"Company Name\")\n",
    "    stock_ticker: str = Field(description=\"Company stock ticker\")\n",
    "    latest_price: float = Field(description=\"The stock price at closing\")\n",
    "\n",
    "\n",
    "response = client.beta.chat.completions.parse(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "      {\"role\": \"system\", \"content\": \"You are an excellent financial news information extractor that returns JSON.\"}, \n",
    "      {\"role\": \"user\", \"content\": f\"Extract the company, stock ticker, and latest price from this newswire.\\nNewswire:\\n{news_wire}\"},\n",
    "    ],\n",
    "    response_format=FinancialStockTicker,\n",
    ")\n",
    "ticker_result = response.choices[0].message.parsed\n",
    "ticker_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fbfe4612-f19c-4eca-9895-262ab36901cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Netflix'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ticker_result.company"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04562a692fac89d",
   "metadata": {},
   "source": [
    "### V2. Given a LinkedIn PDF extract information and populate an email template using structured output\n",
    "\n",
    "```bash\n",
    "cd email_assistant/v2\n",
    "```\n",
    "\n",
    "If you just want to play:\n",
    "```bash\n",
    "python app.py\n",
    "```\n",
    "\n",
    "If you want to write some code:\n",
    "> Edit app.py to import `from pipeline import build_pipeline` and comment out importing `pipeline_gold`\n",
    "\n",
    "> Edit pipeline.py\n",
    "\n",
    "> Edit test_pipeline.py\n",
    "\n",
    "```bash\n",
    "python app.py\n",
    "pytest test_pipeline.py # or test_pipeline_gold.py\n",
    "```\n",
    "\n",
    "Tasks:\n",
    "  - fill in the structured output\n",
    "  - connect to email template\n",
    "  - talk evaluating performance\n",
    "  - write a unit test & discuss testing strategies / thoughts. Guardrails vs evaluations.\n",
    "\n",
    "#### Recap:\n",
    "\n",
    "1. This is a pipeline\n",
    "2. Structured output\n",
    "3. Testing is trickier -- need \"guardrails\" and \"evaluations\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bc77fb-8cb8-4ca0-930b-ba2e64e916f2",
   "metadata": {},
   "source": [
    "### V3. Adding in agent/agentic + Human-in-the-loops\n",
    "\n",
    "\n",
    "```bash\n",
    "cd email_assistant/v3\n",
    "```\n",
    "\n",
    "If you just want to play:\n",
    "```bash\n",
    "python app.py\n",
    "```\n",
    "\n",
    "If you want to write some code:\n",
    "> Edit app.py to import `from pipeline import build_pipeline` and comment out importing `pipeline_gold`\n",
    "\n",
    "> Edit pipeline.py\n",
    "\n",
    "```bash\n",
    "python app.py\n",
    "```\n",
    "\n",
    "\n",
    "Tasks:\n",
    "  - add in conversation history \n",
    "  - play with the app\n",
    "  - compare contrast with V2 from a productionization standpoint \n",
    "  - how to evaluate / test? (user edits?)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751ebc31-3191-4f31-bee0-321dcb9133eb",
   "metadata": {},
   "source": [
    "### V4. Ideas for extensions - for those that need it ->\n",
    "- Adjust to provide a job description and ask the LLM for a fit\n",
    "   - use CoT and other prompting techniques\n",
    "- RAG: searching for a candidate given job reqs to send email to\n",
    "- Function calling: get the weather given the location of the candidate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f9bebd0bf17c54",
   "metadata": {},
   "source": [
    "# Shapes of LLM usage: pipeline vs agentic + Human in the loop\n",
    "\n",
    "## What is a pipeline?\n",
    "\n",
    "\n",
    "## What is an agent? What is agentic? \n",
    "\n",
    "## What is Human in the loop?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc550b8dcbfb1285",
   "metadata": {},
   "source": [
    "# A start to an SDLC - by building out an app\n",
    "\n",
    "![Alt text](img/3-llm-virtuous-cycle.png)\n",
    "\n",
    "## Testing & evaluations\n",
    "\n",
    "At a high level here are the things to think about.\n",
    "\n",
    "### Dimensions:\n",
    "#### micro \n",
    "You need to understand what each LLM call is doing. \n",
    "Easy for one, but picutre 60+ LLM calls.\n",
    "\n",
    "#### macro\n",
    "You need to place things into the context of the business and measure it. This is arguably most important else you have no way to measure efficacy or impact.\n",
    "\n",
    "#### Guardrails\n",
    "Help you keep / define the shape of your application. What you would likely run in CI.\n",
    "\n",
    "#### Evaluations\n",
    "You need to label & measure how it's performing in production. Examples here get added to guardrails.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae6c98cc4f5c7c5",
   "metadata": {},
   "source": [
    "# TODOs:\n",
    "\n",
    " - code to ingest eval output from burr (skip?)\n",
    " - make sure it all works on codespaces\n",
    " - slide with QR codes (repo, oai, course, burr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f5284cb27282b6d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T04:14:07.982819Z",
     "start_time": "2024-11-08T04:14:07.953518Z"
    }
   },
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed08fed18a37684",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
